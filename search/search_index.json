{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"Welcome to My CS 188 Learning Notes","text":"<p>Hello! This is Hongyi Chen's personal knowledge base.</p> <p>This site was created to systematically organize and review the concepts I'm learning during my studies at UC Berkeley and beyond.</p> <p>The core focus is on my course notes, especially for CS 188 (Introduction to Artificial Intelligence). My goal is to build a solid and clear knowledge structure through continuous documentation, organization, and review.</p>"},{"location":"#about-this-site","title":"About This Site","text":"<p>How This Site Is Built</p> <p>This website is built and maintained by me using the following technologies:</p> <ul> <li>Content: Written in plain <code>Markdown</code></li> <li>Site Generator: <code>MkDocs</code> with the <code>Material for MkDocs</code> theme</li> <li>Hosting &amp; CI/CD: <code>Git</code> &amp; <code>GitHub Pages</code> for automated deployment</li> </ul> <p>This setup allows me to focus purely on the content of my notes, while the formatting, building, and publishing process is fully automated and highly efficient.</p> <p>\"Stay hungry. Stay foolish.\" - Steve Jobs</p>"},{"location":"Lecture%2001%20Uninformed%20Search/","title":"Lecture 01 Uninformed Search","text":""},{"location":"Lecture%2001%20Uninformed%20Search/#cs-180-introduction-to-ai","title":"CS 180 - Introduction to AI","text":""},{"location":"Lecture%2001%20Uninformed%20Search/#lecture-1-search","title":"Lecture 1: Search","text":""},{"location":"Lecture%2001%20Uninformed%20Search/#1-different-agents","title":"1. Different Agents","text":"<ul> <li>Reflex agents:  based on current percept (and maybe memory).</li> <li>Planning agents: based on (hypothesized) consequences of actions.</li> </ul>"},{"location":"Lecture%2001%20Uninformed%20Search/#2-search-problem","title":"2. Search Problem \u641c\u7d22\u95ee\u9898","text":"<p>\u4e00\u4e2a\u641c\u7d22\u95ee\u9898\u5fc5\u987b\u5305\u542b\u4ee5\u4e0b\u5143\u7d20\uff1a * A state space - The set of all possible states that are possible in your given world * A set of actions available in each state * A transition model - Outputs the next state when a specific action is taken at current state * An action cost - Incurred when moving from one state to another after applying an action * A start state - The state in which an agent exists initially * A goal test - A function that takes a state as input, and determines whether it is a goal state</p> <p>A solution is a sequence of actions (a plan) which transforms the start state to a goal state.</p>"},{"location":"Lecture%2001%20Uninformed%20Search/#21-state-space","title":"2.1. State Space","text":"<p>\u89e3\u51b3\u95ee\u9898\u65e0\u9700\u6240\u6709\u7ec6\u8282</p> <ul> <li>World State: includes every last detail of the environment.</li> <li>Search State: keeps only the details needed for planning.</li> </ul>"},{"location":"Lecture%2001%20Uninformed%20Search/#22-state-space-graphs","title":"2.2. State Space Graphs","text":"<p>A mathematical representation of a search problem. </p> <p>(We can rarely build this full graph in memory - Too Big!!!)</p> <ul> <li>Nodes - (abstracted) world configurations</li> <li>Arcs - successors (actions)</li> <li>The goal test is a set of goal nodes (maybe only one)</li> </ul> <p>\u641c\u7d22\u95ee\u9898\u5728\u4e8e\u5bfb\u627e\u4ece\u521d\u59cb\u72b6\u6001\u5230\u76ee\u6807\u72b6\u6001\u7684\u8def\u5f84\u3002</p> <p>\u6240\u6709\u53ef\u80fd\u4f4d\u7f6e\u548c\u9053\u8def\u6784\u6210\u7684\u5b8c\u6574\u5730\u56fe\u5373\u4e3a\u72b6\u6001\u7a7a\u95f4\u56fe\u3002</p>"},{"location":"Lecture%2001%20Uninformed%20Search/#23-search-tree","title":"2.3. Search Tree","text":"<p>General Procedure <pre><code>function Tree-Search(problem,strategy) -&gt; returns a solution, or failure\n    initialize the search tree using the initial state of problem\n    loop do\n        if no candidates for expansion:\n            return failure\n\n        choose a leaf node for expansion according to startegy\n        if node contains a goal state:\n            return the corresponding solution\n        else:\n            expand the node and add the resulting nodes to the search tree\n</code></pre></p> <p></p> <p>\u5982\u679c\u8fd9\u4e9b\u7ed3\u6784\u4f53\u8fc7\u5927\u800c\u65e0\u6cd5\u5728\u5185\u5b58\u4e2d\u8868\u793a\u600e\u4e48\u529e\uff1f \u4ec5\u50a8\u5b58\u5f53\u524d\u72b6\u6001\uff01</p> <p>Similarity \u6c38\u8fdc\u65e0\u6cd5\u5728\u5185\u5b58\u4e2d\u6784\u5efa\u5b8c\u6574\u7684\u641c\u7d22\u6811\uff0c\u56e0\u4e3a\u5b83\u4f1a\u53d8\u5f97\u8fc7\u4e8e\u5e9e\u5927</p> <p>Each NODE in the search tree is an entire PATH in the state space graph. \u641c\u7d22\u6811\u4e2d\u7684\u8282\u70b9\u4e0d\u4ec5\u8868\u793a\u4e00\u4e2a\u4f4d\u7f6e\uff0c\u66f4\u4ee3\u8868\u5230\u8fbe\u8be5\u4f4d\u7f6e\u6240\u91c7\u53d6\u7684\u5b8c\u6574\u52a8\u4f5c\u5e8f\u5217\u3002</p>"},{"location":"Lecture%2001%20Uninformed%20Search/#3-uninformed-search-methods","title":"3. Uninformed Search Methods","text":"<p>\u5f53\u6211\u4eec\u5728\u641c\u7d22\u6811\u4e2d\u65e0\u6cd5\u786e\u5b9a\u76ee\u6807\u72b6\u6001\u7684\u4f4d\u7f6e\u65f6\uff0c\u5c31\u4e0d\u5f97\u4e0d\u4ece\u65e0\u4fe1\u606f\u641c\u7d22\u8303\u7574\u5185\u7684\u6280\u672f\u4e2d\u9009\u62e9\u6811\u641c\u7d22\u7b56\u7565\u3002 \u8fd9\u5305\u542b\u4e86\u4e09\u4e2a\u5e38\u89c1\u7684\u7b97\u6cd5\uff1a DFS, BFS, UCS\u3002</p>"},{"location":"Lecture%2001%20Uninformed%20Search/#31-depth-first-search-dfs","title":"3.1. Depth-First Search (DFS)","text":"\u7279\u6027 \u63cf\u8ff0 \u601d\u8def \u4ece\u8d77\u59cb\u8282\u70b9\u9009\u62e9\u6700\u6df1\u7684\u8fb9\u754c\u8282\u70b9\u8fdb\u884c\u6269\u5c55\u3002 \u8fb9\u754c \u79fb\u9664\u6700\u6df1\u8282\u70b9\u5e76\u7528\u5176\u5b50\u8282\u70b9\u66ff\u6362\u3002\u8fd9\u540c\u65f6\u610f\u5473\u7740\u5b50\u8282\u70b9\u73b0\u5728\u6210\u4e3a\u65b0\u7684\u6700\u6df1\u8282\u70b9\u2014\u2014\u5b83\u4eec\u7684\u6df1\u5ea6\u6bd4\u4e4b\u524d\u6700\u6df1\u8282\u70b9\u7684\u6df1\u5ea6\u59271\u3002\u4f7f\u7528 Stack \u5f88\u5bb9\u6613\u80fd\u5b8c\u6210\uff01 \u5b8c\u6574\u6027 \u4e0d\u5b8c\u5168\u3002\u5982\u679c\u5b58\u5728\u5faa\u73af\uff0c\u76f8\u5e94\u7684\u641c\u7d22\u6811\u6df1\u5ea6\u4f1a\u65e0\u9650\u3002 \u6700\u4f18\u6027 \u4e0d\u5177\u5907\u3002\u53ea\u662f\u5728\u641c\u7d22\u6811\u4e2d\u627e\u5230\u201c\u6700\u5de6\u8fb9\u201d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u4e0d\u8003\u8651\u8def\u5f84\u6210\u672c\u3002 \u65f6\u95f4\u590d\u6742\u5ea6 \\(O(b^m)\\), \\(m\\) is the maximum depth, \\(b\\) is the branching factor \uff08\u4e00\u4e2a\u8282\u70b9\u6700\u591a\u6709\u591a\u5c11\u4e2a\u5b50\u8282\u70b9\uff09. \u7a7a\u95f4\u590d\u6742\u5ea6 \\(O(bm)\\) <p>\u4f8b\uff1a\u5728\u4e00\u4e2a\u56fe\u4e2d\u7528 DFS \u627e\u5230\u4ece\u8282\u70b9 A \u5230 G \u7684\u8def\u5f84 <pre><code>              (A)\n               |\n        +------+------+\n        |             |\n       (B)           (C)\n        |             |\n     +--+--+          |\n     |     |          |\n    (D)   (E)        (F)\n           |          |\n           |          |\n           +---+------+\n               |\n              (G)\n</code></pre></p> <p>\u903b\u8f91\uff1a\u4f7f\u7528\u4e24\u4e2a\u6570\u636e\u7ed3\u6784\u5b9e\u73b0 * \u8fb9\u754c (Frontier)\uff1a\u7528\u4e00\u4e2a\u6808 (Stack) \u6765\u5b9e\u73b0\u3002\u6808\u9876\u6c38\u8fdc\u662f\u6211\u4eec\u4e0b\u4e00\u6b65\u8981\u63a2\u7d22\u7684\u8282\u70b9\u3002 * \u5df2\u8bbf\u95ee (Visited)\uff1a\u7528\u4e00\u4e2a\u96c6\u5408 (Set) \u6765\u8bb0\u5f55\u5df2\u7ecf\u63a2\u7d22\u8fc7\u7684\u8282\u70b9\uff0c\u9632\u6b62\u8d70\u56de\u5934\u8def\u6216\u9677\u5165\u6b7b\u5faa\u73af\u3002</p> <p>Step 0: \u5c06\u8d77\u70b9 A \u653e\u5165\u6808\u4e2d\u3002  * Frontier (Top -&gt; Bottom): [A]  Visited: {} Step1: \u63a2\u7d22 A \u4ece\u6808\u9876\u53d6\u51fa A\u3002A \u4e0d\u662f\u76ee\u6807\u3002\u5c06 A \u6807\u8bb0\u4e3a\u201c\u5df2\u8bbf\u95ee\u201d\u3002\u627e\u5230 A \u7684\u6240\u6709\u90bb\u5c45 (B, C)\uff0c\u5c06\u5b83\u4eec\u538b\u5165\u6808\u4e2d\u3002 *  Frontier: [B, C] *  Visited: {A} Step 2: \u63a2\u7d22 B \u4ece\u6808\u9876\u53d6\u51fa B\u3002B \u4e0d\u662f\u76ee\u6807\u3002\u5c06 B \u6807\u8bb0\u4e3a\u201c\u5df2\u8bbf\u95ee\u201d\u3002\u627e\u5230 B \u7684\u90bb\u5c45 (D, E)\uff0c\u5c06\u5b83\u4eec\u538b\u5165\u6808\u4e2d\u3002 * Frontier: [D, E, C] * Visited: {A, B} Step 3: \u63a2\u7d22 D (\u649e\u5357\u5899) \u4ece\u6808\u9876\u53d6\u51fa D\u3002D \u4e0d\u662f\u76ee\u6807\u3002\u5c06 D \u6807\u8bb0\u4e3a\u201c\u5df2\u8bbf\u95ee\u201d\u3002D \u6ca1\u6709\u4efb\u4f55\u90bb\u5c45\uff0c\u662f\u4e2a\u6b7b\u80e1\u540c\u3002 * Frontier: [E, C] * Visited: {A, B, D} Step 4: \u63a2\u7d22E * Frontier\uff1a[G, C] * Visited: {A, B, D, E} Step 5:* \u63a2\u7d22G (\u627e\u5230\u76ee\u6807\uff01) \u4ece\u6808\u9876\u53d6\u51fa G\u3002G \u662f\u6211\u4eec\u7684\u76ee\u6807\uff01\u641c\u7d22\u6210\u529f\uff01</p>"},{"location":"Lecture%2001%20Uninformed%20Search/#32-breadth-first-search-bfs","title":"3.2. Breadth-First Search (BFS)","text":"\u7279\u6027 \u63cf\u8ff0 \u601d\u8def \u9009\u62e9\u8ddd\u79bb\u8d77\u59cb\u8282\u70b9\u6700\u6d45\u7684\u8fb9\u754c\u8282\u70b9\u8fdb\u884c\u6269\u5c55\u3002 \u8fb9\u754c \u5148\u8bbf\u95ee\u8f83\u6d45\u7684\u8282\u70b9\uff0c\u7136\u540e\u518d\u8bbf\u95ee\u8f83\u6df1\u7684\u8282\u70b9\uff0c\u6309\u7167\u8282\u70b9\u7684\u63d2\u5165\u987a\u5e8f\u8fdb\u884c\u8bbf\u95ee\u3002\u4f7f\u7528Queue\u5f88\u5bb9\u6613\u5b9e\u73b0\uff01 \u5b8c\u6574\u6027 \u5b8c\u5168\u3002\u5c42\u6570\u6709\u9650\uff0c\u5fc5\u7136\u5230\u8fbe\u6700\u6df1\u5904\u3002 \u6700\u4f18\u6027 \u4e0d\u5177\u5907\u3002\u5728\u786e\u5b9a\u8fb9\u754c\u4e0a\u8981\u66ff\u6362\u7684\u8282\u70b9\u65f6\u6839\u672c\u4e0d\u8003\u8651\u6210\u672c\u3002BFS \u4fdd\u8bc1\u6700\u4f18\u7684\u7279\u6b8a\u60c5\u51b5\u662f\u6240\u6709\u8fb9\u6210\u672c\u76f8\u7b49\uff0c\u56e0\u4e3a\u8fd9\u4f1a\u5c06 BFS \u7b80\u5316\u4e3a\u5747\u5300\u6210\u672c\u641c\u7d22\u7684\u7279\u4f8b\u3002 \u65f6\u95f4\u590d\u6742\u5ea6 \\(O(b^s)\\), \\(s\\) \u4e3a\u6700\u4f18\u89e3\u7684\u6df1\u5ea6, \\(b\\) is the branching factor \uff08\u4e00\u4e2a\u8282\u70b9\u6700\u591a\u6709\u591a\u5c11\u4e2a\u5b50\u8282\u70b9\uff09. \u7a7a\u95f4\u590d\u6742\u5ea6 \\(O(b^s)\\)"},{"location":"Lecture%2001%20Uninformed%20Search/#33-uniform-cost-search-ucs","title":"3.3. Uniform Cost Search (UCS)","text":"\u7279\u6027 \u63cf\u8ff0 \u601d\u8def \u59cb\u7ec8\u4ece\u8d77\u59cb\u8282\u70b9\u9009\u62e9\u6210\u672c\u6700\u4f4e\u7684\u8fb9\u754c\u8282\u70b9\u8fdb\u884c\u6269\u5c55\u3002 \u8fb9\u754c \u9009\u62e9\u57fa\u4e8e\u5806\u7684Priority Queue\u3002\u5165\u961f\u8282\u70b9\\(v\\)\u7684\u4f18\u5148\u7ea7\u4e3a\u8d77\u70b9\u8282\u70b9\u5230\\(v\\)\u7684\u8def\u5f84\u6210\u672c\uff0c\u6216\\(v\\)\u7684\u540e\u5411\u6210\u672c\u3002\u5728\u79fb\u9664\u5f53\u524d\u6700\u5c0f\u6210\u672c\u8def\u5f84\u5e76\u66ff\u6362\u4e3a\u5176\u5b50\u8282\u70b9\u65f6\uff0c\u4f1a\u81ea\u52a8\u91cd\u65b0\u6392\u5e8f\u4ee5\u7ef4\u6301\u57fa\u4e8e\u8def\u5f84\u6210\u672c\u7684\u671f\u671b\u6392\u5e8f\u3002 \u5b8c\u6574\u6027 \u5b8c\u5168\u3002\u5982\u679c\u76ee\u6807\u72b6\u6001\u5b58\u5728\uff0c\u5b83\u5fc5\u7136\u5b58\u5728\u4e00\u6761\u6709\u9650\u957f\u5ea6\u7684\u6700\u77ed\u8def\u5f84. \u6700\u4f18\u6027 \u5177\u5907\u3002(\u7edf\u4e00\u6210\u672c\u641c\u7d22\u91c7\u7528\u7684\u7b56\u7565\u4e0e Dijkstra \u7b97\u6cd5\u76f8\u540c\uff0c\u4e3b\u8981\u533a\u522b\u5728\u4e8e UCS \u4f1a\u5728\u627e\u5230\u89e3\u72b6\u6001\u65f6\u7ec8\u6b62\uff0c\u800c\u4e0d\u662f\u627e\u5230\u5230\u8fbe\u6240\u6709\u72b6\u6001\u7684\u6700\u77ed\u8def\u5f84\u3002) \u65f6\u95f4\u590d\u6742\u5ea6 \\(O(b^{C^*/\\epsilon})\\), \\(C^*\\) \u4e3a\u4ece\u8d77\u70b9\u5230\u7ec8\u70b9\u7684\u6700\u4f18\u8def\u5f84\u7684\u5b9e\u9645\u6210\u672c, \\(\\epsilon\\) \u4e3a\u4efb\u610f\u4e00\u6b65\uff08\u8fb9\uff09\u7684\u6700\u5c0f\u6210\u672c\u3002 \u7a7a\u95f4\u590d\u6742\u5ea6 \\(O(b^{C^*/\\epsilon})\\) <p><pre><code>           (A) &lt;-- \u8d77\u70b9\n          /   \\\n         /     \\\n      (1)       (5)\n       /         \\\n      v           v\n     (B)         (C)\n      \\           /\n       \\         /\n      (3)       (1)\n         \\     /\n          v   v\n           (D)\n            |\n           (2)\n            |\n            v\n           (G) &lt;-- \u76ee\u6807\n</code></pre> \u903b\u8f91\uff1a * \u8fb9\u754c (Frontier)\uff1a\u7528\u4e00\u4e2a\u4f18\u5148\u961f\u5217 (Priority Queue) \u5b9e\u73b0\u3002\u961f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u662f (\u6210\u672c, \u8282\u70b9)\uff0c\u5e76\u59cb\u7ec8\u6309\u6210\u672c\u4ece\u5c0f\u5230\u5927\u6392\u5e8f\u3002 * \u5df2\u63a2\u7d22 (Explored)\uff1a\u7528\u4e00\u4e2a\u96c6\u5408 (Set) \u6765\u8bb0\u5f55\u5df2\u7ecf\u63a2\u7d22\u8fc7\u7684\u8282\u70b9\uff0c\u786e\u4fdd\u6211\u4eec\u4e0d\u4f1a\u91cd\u590d\u5904\u7406\u3002</p> <p>Step 0: \u521d\u59cb\u5316 \u5c06\u8d77\u70b9 A \u653e\u5165\u4f18\u5148\u961f\u5217\uff0c\u5176\u6210\u672c\u4e3a0\u3002 * Frontier (\u6309\u6210\u672c\u6392\u5e8f): [(0, A)] * Explored: {}</p> <p>Step 1: \u63a2\u7d22 A \u4ece\u4f18\u5148\u961f\u5217\u4e2d\u53d6\u51fa\u6210\u672c\u6700\u4f4e\u7684\u8282\u70b9 (0, A)\u3002\u5c06 A \u653e\u5165\u201c\u5df2\u63a2\u7d22\u201d\u3002\u627e\u5230 A \u7684\u90bb\u5c45 B \u548c C\u3002</p> <p>\u5230 B \u7684\u6210\u672c\u662f 0 + 1 = 1\u3002\u5c06 (1, B) \u653e\u5165\u961f\u5217\u3002 \u5230 C \u7684\u6210\u672c\u662f 0 + 5 = 5\u3002\u5c06 (5, C) \u653e\u5165\u961f\u5217\u3002 * Frontier: [(1, B), (5, C)] * Explored: {A}</p> <p>Step 2: \u63a2\u7d22 B (\u800c\u4e0d\u662f C\uff01) \u6b64\u65f6\u961f\u5217\u4e2d\u6210\u672c\u6700\u4f4e\u7684\u662f (1, B)\u3002\u53d6\u51fa\u5b83\uff0c\u5e76\u5c06 B \u653e\u5165\u201c\u5df2\u63a2\u7d22\u201d\u3002\u627e\u5230 B \u7684\u90bb\u5c45 D\u3002</p> <p>\u5230 D \u7684\u6210\u672c\u662f (A\u5230B\u7684\u6210\u672c) + (B\u5230D\u7684\u6210\u672c) = 1 + 3 = 4\u3002\u5c06 (4, D) \u653e\u5165\u961f\u5217\u3002</p> <ul> <li>Frontier: [(4, D), (5, C)]  (\u6ce8\u610f (4,D) \u6392\u5728\u4e86 (5,C) \u7684\u524d\u9762)</li> <li>Explored: {A, B}</li> </ul> <p>\u5173\u952e\u70b9\uff1a \u5c3d\u7ba1 C \u662f A \u7684\u76f4\u63a5\u90bb\u5c45\uff0c\u4f46\u56e0\u4e3a\u5f53\u524d\u5230 D \u7684\u5df2\u77e5\u8def\u5f84\u6210\u672c(4)\u6bd4\u5230 C \u7684\u6210\u672c(5)\u66f4\u4f4e\uff0c\u6240\u4ee5UCS\u4f18\u5148\u9009\u62e9\u4e86\u8fd9\u6761\u770b\u8d77\u6765\u66f4\u6709\u5e0c\u671b\u7684\u8def\u5f84\u3002</p> <p>Step 3: \u63a2\u7d22 D \u4ece\u4f18\u5148\u961f\u5217\u4e2d\u53d6\u51fa\u6210\u672c\u6700\u4f4e\u7684\u8282\u70b9 (4, D)\u3002\u5c06 D \u653e\u5165\u201c\u5df2\u63a2\u7d22\u201d\u3002\u627e\u5230 D \u7684\u90bb\u5c45 G\u3002</p> <p>\u5230 G \u7684\u6210\u672c\u662f (A\u5230D\u7684\u6210\u672c) + (D\u5230G\u7684\u6210\u672c) = 4 + 2 = 6\u3002\u5c06 (6, G) \u653e\u5165\u961f\u5217\u3002</p> <ul> <li>Frontier: [(5, C), (6, G)]</li> <li>Explored: {A, B, D}</li> </ul> <p>Step 4: \u63a2\u7d22 C \u4ece\u4f18\u5148\u961f\u5217\u4e2d\u53d6\u51fa\u6210\u672c\u6700\u4f4e\u7684\u8282\u70b9 (5, C)\u3002\u5c06 C \u653e\u5165\u201c\u5df2\u63a2\u7d22\u201d\u3002\u627e\u5230 C \u7684\u90bb\u5c45 D\u3002</p> <p>\u5230 D \u7684\u65b0\u8def\u5f84\u6210\u672c\u662f (A\u5230C\u7684\u6210\u672c) + (C\u5230D\u7684\u6210\u672c) = 5 + 1 = 6\u3002</p> <p>\u68c0\u67e5\u53d1\u73b0 D \u5df2\u7ecf\u5728\u201c\u5df2\u63a2\u7d22\u201d\u96c6\u5408\u91cc\u4e86\u3002\u800c\u4e14\u65b0\u8def\u5f84\u6210\u672c(6)\u5e76\u4e0d\u6bd4\u4e4b\u524d\u5230 D \u7684\u6210\u672c(4)\u66f4\u4f4e\uff0c\u56e0\u6b64\u6211\u4eec\u5ffd\u7565\u8fd9\u6761\u8def\u5f84\u3002</p> <ul> <li>Frontier: [(6, G)]</li> <li>Explored: {A, B, D, C}</li> </ul> <p>Step 5: \u63a2\u7d22 G (\u627e\u5230\u76ee\u6807\uff01) \u4ece\u4f18\u5148\u961f\u5217\u4e2d\u53d6\u51fa\u6210\u672c\u6700\u4f4e\u7684\u8282\u70b9 (6, G)\u3002 G \u662f\u6211\u4eec\u7684\u76ee\u6807\uff01\u641c\u7d22\u6210\u529f\uff01 \u627e\u5230\u7684\u6700\u4f4e\u6210\u672c\u662f 6\uff0c\u8def\u5f84\u4e3a A -&gt; B -&gt; D -&gt; G\u3002</p>"},{"location":"Lecture%2002%20Informed%20Search/","title":"Lecture 02 Informed Search","text":""},{"location":"Lecture%2002%20Informed%20Search/#cs-180-introduction-to-ai","title":"CS 180 - Introduction to AI","text":""},{"location":"Lecture%2002%20Informed%20Search/#lecture-2-informed-search","title":"Lecture 2: Informed Search","text":"<p>\u8fd9\u548cUninformed Search\u6709\u4ec0\u4e48\u533a\u522b\uff1f \u5229\u7528\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u989d\u5916\u4fe1\u606f\uff0c\u4f18\u5148\u63a2\u7d22\u201c\u770b\u8d77\u6765\u201d\u79bb\u76ee\u6807\u6700\u8fd1\u7684\u8282\u70b9\u3002 \u4e0d\u8fc7\uff0c\u5b9e\u9645\u4e0a\u672a\u5fc5\u4f1a\u66f4\u8fd1\uff01</p>"},{"location":"Lecture%2002%20Informed%20Search/#1-heuristics","title":"1. Heuristics","text":"<p>A heuristic is a function that estimates how close a state is to a goal.</p> <p>\u542f\u53d1\u5f0f\u7b97\u6cd5\u901a\u5e38\u662f\u9488\u5bf9\u5bbd\u677e\u95ee\u9898\uff08\u5373\u79fb\u9664\u4e86\u539f\u59cb\u95ee\u9898\u7684\u4e00\u4e9b\u7ea6\u675f\uff09\u7684\u89e3\u51b3\u65b9\u6848</p>"},{"location":"Lecture%2002%20Informed%20Search/#2-greedy-search","title":"2. Greedy Search \u8d2a\u5a6a\u641c\u7d22","text":"\u7279\u6027 \u63cf\u8ff0 \u601d\u8def \u9009\u62e9\u5177\u6709\u6700\u4f4e\u542f\u53d1\u5f0f\u503c\u7684\u8fb9\u754c\u8282\u70b9\u8fdb\u884c\u6269\u5c55\u3002 \u8fb9\u754c \u91c7\u7528Priority Queue\u3002\u533a\u522b\u5728\u4e8e\uff0c\u8d2a\u5a6a\u641c\u7d22\u4e0d\u662f\u4f7f\u7528\u540e\u5411\u6210\u672c\uff08\u4ece\u8d77\u70b9\u8d70\u5230\u5f53\u524d\u8282\u70b9 \\(n\\) \u6240\u5df2\u7ecf\u82b1\u8d39\u7684\u5b9e\u9645\u6210\u672c\uff09\u6765\u5206\u914d\u4f18\u5148\u7ea7\uff0c\u800c\u662f\u4f7f\u7528\u542f\u53d1\u5f0f\u503c\u5f62\u5f0f\u7684\u9884\u4f30\u524d\u5411\u6210\u672c\uff08\u4ece\u5f53\u524d\u8282\u70b9 n \u8d70\u5230\u7ec8\u70b9\u6240\u9884\u4f30\u7684\u5269\u4f59\u6210\u672c\uff09\u3002 \u5b8c\u6574\u6027 \u4e0d\u5b8c\u5168\u3002\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8868\u73b0\u901a\u5e38\u76f8\u5f53\u96be\u4ee5\u9884\u6d4b\u3002 \u6700\u4f18\u6027 \u4e0d\u5177\u5907\u3002\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8868\u73b0\u901a\u5e38\u76f8\u5f53\u96be\u4ee5\u9884\u6d4b\u3002 <p>\u5bf9\u6bd4UCS\u548cGreedy Search \u4ece\u57ce\u5e02 S (\u8d77\u70b9) \u524d\u5f80\u57ce\u5e02 G (\u7ec8\u70b9)\u3002\u9014\u4e2d\u6709\u4e24\u4e2a\u4e2d\u8f6c\u57ce\u5e02 A \u548c B\u3002 <pre><code>      (cost=5)\n   A --------- G\n  /           /\n(cost=4)   (cost=2)\n/           /\nS --------- B\n  (cost=2)\n</code></pre> \u7ed9\u5b9a Heuristics \uff08\u8d2a\u5a6a\u641c\u7d22\u9700\u8981\uff0cUCS\u4e0d\u9700\u8981\uff09</p> <p>\u6211\u4eec\u7528\u201c\u5230\u7ec8\u70b9G\u7684\u76f4\u7ebf\u8ddd\u79bb\u201d\u4f5c\u4e3a\u542f\u53d1\u5f0f\u51fd\u6570 h(n) \u7684\u503c\u3002 $$ h(A) = 1   \\ h(B) = 4  \\ h(S) = 5   \\ h(G) = 0  $$</p> \u7279\u6027 UCS (\u53ea\u5173\u5fc3\u540e\u5411\u6210\u672c) Greedy Search (\u53ea\u5173\u5fc3\u542f\u53d1\u503c) \u6838\u5fc3\u5173\u6ce8\u70b9 \u8def\u5f84\u5df2\u82b1\u8d39\u7684\u5b9e\u9645\u6210\u672c \\(g(n)\\) \u5230\u8fbe\u76ee\u6807\u7684\u4f30\u8ba1\u6210\u672c \\(h(n)\\) \u51b3\u7b56\u8fc7\u7a0b UCS\u53d1\u73b0\u53bbB\u7684\u6210\u672c\u6bd4\u53bbA\u7684\u6210\u672c\u66f4\u4f4e\uff0c\u9009\u62e9\u6269\u5c55B\uff1b\u968f\u540e\u53d1\u73b0G\u662f\u7ec8\u70b9\uff0c\u641c\u7d22\u7ed3\u675f\u3002 \u8d2a\u5a6a\u641c\u7d22\u53d1\u73b0A\u7684\u542f\u53d1\u503c\u8fdc\u4f4e\u4e8eB\u7684\u542f\u53d1\u503c\uff0c\u9009\u62e9\u6269\u5c55A\uff1b\u968f\u540e\u53d1\u73b0G\u662f\u7ec8\u70b9\uff0c\u641c\u7d22\u7ed3\u675f\u3002 \u627e\u5230\u7684\u8def\u5f84 S \u2192 B \u2192 G S \u2192 A \u2192 G \u6700\u7ec8\u8def\u5f84\u603b\u6210\u672c 4 9 \u8def\u5f84\u7ed3\u679c \u6700\u4f18\u8def\u5f84 \u975e\u6700\u4f18\u8def\u5f84"},{"location":"Lecture%2002%20Informed%20Search/#3-a-search","title":"3. A* Search","text":"\u7279\u6027 \u63cf\u8ff0 \u601d\u8def \u9009\u62e9\u5177\u6709\u6700\u4f4e\u4f30\u8ba1\u603b\u6269\u5c55\u6210\u672c\u7684\u8fb9\u754c\u8282\u70b9 \u8fb9\u754c \u91c7\u7528Priority Queue\u3002A*\u5c06 UCS \u4f7f\u7528\u7684\u603b\u540e\u5411\u6210\u672c\u4e0e\u8d2a\u5a6a\u641c\u7d22\u4f7f\u7528\u7684\u4f30\u8ba1\u524d\u5411\u6210\u672c\uff08Heuristics\u503c\uff09\u76f8\u52a0\uff0c\u4ece\u800c\u6709\u6548\u5730\u5f97\u51fa\u4ece\u8d77\u70b9\u5230\u76ee\u6807\u7684\u4f30\u8ba1\u603b\u6210\u672c. \u5b8c\u6574\u6027&amp;\u6700\u4f18\u6027 \u5728\u7ed9\u5b9a\u5408\u9002\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\uff08\u6211\u4eec\u7a0d\u540e\u4f1a\u4ecb\u7ecd\uff09\u7684\u60c5\u51b5\u4e0b\uff0cA* \u641c\u7d22\u65e2\u5b8c\u5907\u53c8\u6700\u4f18\u3002"},{"location":"Lecture%2002%20Informed%20Search/#4-admissibility-heuristics","title":"4. Admissibility \u53ef\u63a5\u7eb3\u884c \uff08\u5bfb\u627e\u597d\u7684Heuristics\uff09","text":"<p>Recall\uff1a\u9996\u5148\u91cd\u65b0\u8868\u8ff0 UCS\u3001\u8d2a\u5a6a\u641c\u7d22\u548c A \u4e2d\u7528\u4e8e\u786e\u5b9a\u4f18\u5148\u7ea7\u961f\u5217\u987a\u5e8f\u7684\u65b9\u6cd5\u3002 \\(g(n)\\) - Total backwards cost computed by UCS. \\(h(n)\\) - The heuristic value function, or estimated forward cost, used by greedy search. \\(f(n)\\) - Estimated total cost, used by A search. \\(f(n) = g(n) + h(n)\\)</p> <p>\u4f46\u4e0d\u662f\u6240\u6709Heuristics\u90fd\u80fd\u8fbe\u5230\u5b8c\u5907\u548c\u6700\u4f18\u3002\u8fd9\u5f15\u51fa\u4e86 Admissibility \u53ef\u63a5\u7eb3\u6027 \u7684\u6982\u5ff5\uff1a The condition required for optimality when using A tree search is known as admissibility*.</p> <p>\u542f\u53d1\u5f0f\u7b97\u6cd5\u4f30\u8ba1\u7684\u503c\u4e0d\u80fd\u9ad8\u4f30\u5b9e\u9645\u6210\u672c\u3002 \\(\\(\\forall n, 0 \\leq h(n) \\leq h^*(n)\\)\\)</p> <p>Theorem: \u5982\u679c\u542f\u53d1\u5f0f\u51fd\u6570 h \u6ee1\u8db3\u53ef\u63a5\u7eb3\u6027\u7ea6\u675f\uff0c\u5219\u5bf9\u8be5\u641c\u7d22\u95ee\u9898\u4f7f\u7528\u5e26\u6709 h \u7684 A* \u6811\u641c\u7d22\u5c06\u4ea7\u751f\u6700\u4f18\u89e3\u3002</p> <p>\u53ef\u4ee5\u5728\u6b64\u5904\u67e5\u770b\u8bc1\u660e</p>"},{"location":"Lecture%2002%20Informed%20Search/#5-graph-search","title":"5. Graph Search","text":"<p>Intention: \u6811\u641c\u7d22\u7684\u95ee\u9898\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5b83\u53ef\u80fd\u6c38\u8fdc\u627e\u4e0d\u5230\u89e3\uff0c\u5728\u72b6\u6001\u7a7a\u95f4\u56fe\u4e2d\u65e0\u9650\u5faa\u73af\u641c\u7d22</p> <p>Graph Search \u5728\u4f7f\u7528\u60a8\u9009\u62e9\u7684\u641c\u7d22\u65b9\u6cd5\u65f6\uff0c\u7ef4\u62a4\u4e00\u4e2a\u201c\u5df2\u5230\u8fbe\u201d\u7684\u6269\u5c55\u8282\u70b9\u96c6\u5408\u3002\u7136\u540e\uff0c\u786e\u4fdd\u6bcf\u4e2a\u8282\u70b9\u5728\u6269\u5c55\u524d\u5c1a\u672a\u5305\u542b\u5728\u96c6\u5408\u4e2d\uff0c\u5982\u679c\u4e0d\u5728\uff0c\u5219\u5728\u6269\u5c55\u540e\u5c06\u5176\u6dfb\u52a0\u5230\u96c6\u5408\u4e2d\u3002\u5e26\u6709\u8fd9\u79cd\u9644\u52a0\u4f18\u5316\u7684\u6811\u641c\u7d22\u79f0\u4e3a\u56fe\u641c\u7d22\u3002</p> <p>Intention: \u56fe\u641c\u7d22\u4f9d\u7136\u5b58\u5728\u95ee\u9898\uff0c\u4e3e\u4e00\u4e2a\u4f8b\u5b50\u5982\u4e0b\u3002 </p> <p>\u7531\u4e8e\u8282\u70b9 A \u7684\u542f\u53d1\u5f0f\u503c\u8fdc\u5927\u4e8e\u8282\u70b9 B \u7684\u542f\u53d1\u5f0f\u503c\uff0c\u56e0\u6b64\u8282\u70b9 C \u9996\u5148\u6cbf\u7740\u7b2c\u4e8c\u6761\u6b21\u4f18\u8def\u5f84\u6269\u5c55\u4e3a\u8282\u70b9 B \u7684\u5b50\u8282\u70b9\u3002\u7136\u540e\uff0c\u5b83\u88ab\u653e\u5165\u201c\u5df2\u5230\u8fbe\u201d\u96c6\u3002 \u56e0\u6b64\u5f53 A* \u56fe\u641c\u7d22\u5c06\u5176\u4f5c\u4e3a A \u7684\u5b50\u8282\u70b9\u8fdb\u884c\u8bbf\u95ee\u65f6\uff0c\u5b83\u65e0\u6cd5\u91cd\u65b0\u6269\u5c55\u5b83\uff0c\u56e0\u6b64\u5b83\u6c38\u8fdc\u627e\u4e0d\u5230\u6700\u4f18\u89e3\u3002</p> <p>\\(\\rightarrow\\) \u4e0d\u4ec5\u9700\u8981\u68c0\u67e5 A* \u662f\u5426\u5df2\u7ecf\u8bbf\u95ee\u8fc7\u67d0\u4e2a\u8282\u70b9\uff0c\u8fd8\u9700\u8981\u68c0\u67e5\u662f\u5426\u627e\u5230\u4e86\u4e00\u6761\u66f4\u4fbf\u5b9c\u7684\u8def\u5f84\uff01</p>"},{"location":"Lecture%2002%20Informed%20Search/#6-dominance-heuristics","title":"6. Dominance \u4f18\u52bf\u5ea6 \uff08\u6bd4\u8f83Heuristics\u7684\u597d\u574f\uff09","text":"<p>\u600e\u6837\u66f4\u597d\uff1f \\(\\rightarrow\\) \u66f4\u7cbe\u786e\u5730\u4f30\u8ba1\u4ece\u4efb\u4f55\u7ed9\u5b9a\u72b6\u6001\u5230\u76ee\u6807\u7684\u8ddd\u79bb! \u5982\u679c\u542f\u53d1\u5f0f \\(a\\) \u4f18\u4e8e\u542f\u53d1\u5f0f \\(b\\) \uff0c\u90a3\u4e48\u5bf9\u4e8e\u72b6\u6001\u7a7a\u95f4\u56fe\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\uff0c \\(a\\) \u7684\u4f30\u8ba1\u76ee\u6807\u8ddd\u79bb\u90fd\u5927\u4e8e \\(b\\) \u7684\u4f30\u8ba1\u76ee\u6807\u8ddd\u79bb\u3002 \\(\\(\\forall n: h_a(n) \\geq h_b(n)\\)\\)</p> <p>\u901a\u5e38\u7684\u505a\u6cd5\u662f\uff0c\u9488\u5bf9\u4efb\u4f55\u7ed9\u5b9a\u7684\u641c\u7d22\u95ee\u9898\u751f\u6210\u591a\u4e2a\u53ef\u91c7\u7eb3\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5e76\u8ba1\u7b97\u5b83\u4eec\u8f93\u51fa\u503c\u7684\u6700\u5927\u503c.</p>"},{"location":"Lecture%2002%20Informed%20Search/#7-local-search","title":"7. Local Search \u672c\u5730\u641c\u7d22","text":"<p>\u4e0e\u4e4b\u524d\u7684\u4e0d\u540c\uff1a\u4e4b\u524d\u6211\u4eec\u5173\u5fc3\u76ee\u6807\u72b6\u6001\u4ee5\u53ca\u5230\u8fbe\u5b83\u7684\u6700\u4f73\u8def\u5f84\uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u53ea\u5173\u5fc3\u76ee\u6807\u72b6\u6001\u5728\u54ea\u91cc\u3002</p> <p>\u601d\u8def\uff1a\u7b97\u6cd5\u5c40\u90e8\u5730\u5411\u76ee\u6807\u503c\u66f4\u9ad8\u7684\u72b6\u6001\u79fb\u52a8\uff0c\u76f4\u5230\u8fbe\u5230\u6700\u5927\u503c\uff08\u6700\u597d\u662f\u5168\u5c40\u6700\u5927\u503c\uff09</p> <p>\u8fd9\u90e8\u5206\u5305\u542b\u4ee5\u4e0b\u7b97\u6cd5\uff1ahill-climbing, simulated annealing, local beam search, and genetic algorithms.</p> <p>\uff01\uff01\uff01\u5f85\u8865\u5145\uff01\uff01\uff01</p>"},{"location":"Lecture%2003%20CSP/","title":"Lecture 03 CSP","text":""},{"location":"Lecture%2003%20CSP/#cs-180-introduction-to-ai","title":"CS 180 - Introduction to AI","text":""},{"location":"Lecture%2003%20CSP/#lecture-3-constraint-satisfaction-problems","title":"Lecture 3: Constraint Satisfaction Problems","text":"<p>\u4e0e\u641c\u7d22\u95ee\u9898\u4e0d\u540c\uff0cCSP \u662f\u4e00\u79cd\u8bc6\u522b\u95ee\u9898\uff0c\u5373\u6211\u4eec\u53ea\u9700\u8bc6\u522b\u67d0\u4e2a\u72b6\u6001\u662f\u5426\u4e3a\u76ee\u6807\u72b6\u6001\uff0c\u800c\u65e0\u9700\u8003\u8651\u5982\u4f55\u8fbe\u5230\u8be5\u76ee\u6807\u3002</p> <p>\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u662f NP-hard \u7684\uff0c\u5927\u81f4\u610f\u5473\u7740\u4e0d\u5b58\u5728\u5df2\u77e5\u7b97\u6cd5\u53ef\u4ee5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u627e\u5230\u6b64\u7c7b\u95ee\u9898\u7684\u89e3\u3002</p> <p>CSP \u7531\u4e09\u4e2a\u56e0\u7d20\u5b9a\u4e49\uff1a * Variables - CSPs possess a set of \\(N\\) variables \\(X_1, \\ldots, X_N\\) that can each take on a single value from some defined set of values.</p> <ul> <li> <p>Domain - A set \\(\\{x_1, \\ldots, x_d\\}\\) representing all possible values that a CSP variable can take on.</p> </li> <li> <p>Constraints - Constraints define restrictions on the values of variables, potentially with regard to other variables.</p> </li> </ul> <p>\u5178\u578b\u7684\u4f8b\u5b50\u662f \\(N\\) \u7687\u540e\u95ee\u9898\u3002 \u53d8\u91cf - \\(X_{ij}\\)\uff0c\u5176\u4e2d \\(0 \\leq i,j &lt; N\\)\u3002\u6bcf\u4e2a \\(X_{ij}\\) \u4ee3\u8868 \\(N \\times N\\) \u68cb\u76d8\u4e0a\u7684\u4e00\u4e2a\u7f51\u683c\u4f4d\u7f6e\u3002</p> <p>\u57df - \\(\\{0, 1\\}\\)\u3002\u6bcf\u4e2a \\(X_{ij}\\) \u53ef\u4ee5\u53d6\u503c 0 \u6216 1\uff0c\u8fd9\u662f\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u8868\u793a\u5728\u68cb\u76d8\u4f4d\u7f6e \\((i, j)\\) \u662f\u5426\u5b58\u5728\u4e00\u4e2a\u7687\u540e\u3002</p> <p>\u7ea6\u675f - * \\(\\forall i, j, k \\ (X_{ij}, X_{ik}) \\in \\{(0,0), (0,1), (1,0)\\}\\)\u3002\u4efb\u4f55\u4e24\u4e2a\u7687\u540e\u4e0d\u80fd\u5728\u540c\u4e00\u884c\u3002 * \\(\\forall i, j, k \\ (X_{ij}, X_{kj}) \\in \\{(0,0), (0,1), (1,0)\\}\\)\u3002\u4efb\u4f55\u4e24\u4e2a\u7687\u540e\u4e0d\u80fd\u5728\u540c\u4e00\u5217\u3002 * \\(\\forall i, j, k \\ (X_{ij}, X_{i+k, j+k}) \\in \\{(0,0), (0,1), (1,0)\\}\\)\u3002\u4efb\u4f55\u4e24\u4e2a\u7687\u540e\u4e0d\u80fd\u5728\u540c\u4e00\u4e3b\u5bf9\u89d2\u7ebf\u6216\u526f\u5bf9\u89d2\u7ebf\u4e0a\u3002 * \\(\\forall i, j, k \\ (X_{ij}, X_{i+k, j-k}) \\in \\{(0,0), (0,1), (1,0)\\}\\)\u3002\u540c\u4e0a\u3002 * \\(\\sum_{i,j} X_{ij} = N\\)\u3002\u6ee1\u8db3\u68cb\u76d8\u4e0a\u6070\u597d\u6709 \\(N\\) \u4e2a\u7687\u540e\u7684\u8981\u6c42\u3002</p>"},{"location":"Lecture%2003%20CSP/#1-constraints-graph","title":"1. Constraints Graph \u7ea6\u675f\u56fe","text":"<p>\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u901a\u5e38\u8868\u793a\u4e3a\u7ea6\u675f\u56fe\uff0c\u5176\u4e2d\u8282\u70b9\u8868\u793a\u53d8\u91cf\uff0c\u8fb9\u8868\u793a\u53d8\u91cf\u4e4b\u95f4\u7684\u7ea6\u675f\u3002</p> <p>\u7ea6\u675f\u5305\u542b\u4ee5\u4e0b\u51e0\u79cd\u7c7b\u578b\uff1a - Unary Constraints \u4e00\u5143\u7ea6\u675f\uff1a\u6d89\u53ca CSP \u4e2d\u7684\u5355\u4e2a\u53d8\u91cf\u3002\u4e0d\u4f1a\u5728\u7ea6\u675f\u56fe\u4e2d\u8868\u793a\uff0c\u800c\u662f\u5728\u5fc5\u8981\u65f6\u7528\u4e8e\u4fee\u526a\u5176\u7ea6\u675f\u53d8\u91cf\u7684\u57df\u3002 - Binary Constraints \u4e8c\u5143\u7ea6\u675f\uff1a\u4e8c\u5143\u7ea6\u675f\u6d89\u53ca\u4e24\u4e2a\u53d8\u91cf\u3002\u5b83\u4eec\u5728\u7ea6\u675f\u56fe\u4e2d\u4ee5\u4f20\u7edf\u7684\u56fe\u8fb9\u5f62\u5f0f\u8868\u793a\u3002 - Higher-order Constraints \u9ad8\u9636\u7ea6\u675f\uff1a\u6d89\u53ca\u4e09\u4e2a\u6216\u66f4\u591a\u53d8\u91cf\u7684\u7ea6\u675f\u4e5f\u53ef\u4ee5\u7528 CSP \u56fe\u4e2d\u7684\u8fb9\u8868\u793a\uff0c\u5b83\u4eec\u53ea\u662f\u770b\u8d77\u6765\u6709\u70b9\u4e0d\u5bfb\u5e38\u3002</p>"},{"location":"Lecture%2003%20CSP/#2-backtracking-algorithm-solving-csp","title":"2. Backtracking Algorithm (Solving CSP)","text":"<p>\u4f7f\u7528\u56de\u6eaf\u641c\u7d22\u89e3\u51b3\u2014\u2014\u672c\u8d28\u4e0a\u662f\u4f18\u5316\u540e\u7684 DFS\u3002\u6539\u8fdb\u57fa\u4e8e\u4ee5\u4e0b\u539f\u5219\uff1a - \u56fa\u5b9a\u53d8\u91cf\u7684\u987a\u5e8f\uff0c\u5e76\u6309\u6b64\u987a\u5e8f\u9009\u62e9\u53d8\u91cf\u7684\u503c\u3002\uff08\u56e0\u4e3a\u8d4b\u503c\u662f\u53ef\u4ea4\u6362\u7684\uff0c\u6240\u4ee5\u6709\u6548\uff09 - \u4e3a\u53d8\u91cf\u9009\u62e9\u503c\u65f6\uff0c\u4ec5\u9009\u62e9\u4e0e\u4e4b\u524d\u8d4b\u503c\u4e0d\u51b2\u7a81\u7684\u503c\u3002\u5982\u679c\u4e0d\u5b58\u5728\u8fd9\u6837\u7684\u503c\uff0c\u5219\u56de\u6eaf\u5e76\u8fd4\u56de\u5230\u4e0a\u4e00\u4e2a\u53d8\u91cf\uff0c\u5e76\u66f4\u6539\u5176\u503c\u3002</p> <pre><code>function BACKTRACKING-SEARCH(csp) returns solution/failure\n  return RECURSIVE-BACKTRACKING({}, csp)\n\nfunction RECURSIVE-BACKTRACKING(assignment, csp) returns soln/failure\n  if assignment is complete then return assignment\n  var $\\leftarrow$ SELECT-UNASSIGNED-VARIABLE(VARIABLES[csp], assignment, csp)\n  for each value in ORDER-DOMAIN-VALUES(var, assignment, csp) do\n    if value is consistent with assignment given CONSTRAINTS[csp] then\n      add {var = value} to assignment\n      result $\\leftarrow$ RECURSIVE-BACKTRACKING(assignment, csp)\n      if result $\\neq$ failure then return result\n      remove {var = value} from assignment\n  return failure\n</code></pre> <p></p>"},{"location":"Lecture%2003%20CSP/#3-improving-backtracking","title":"3. Improving Backtracking","text":""},{"location":"Lecture%2003%20CSP/#31-filitering","title":"3.1. Filitering \u8fc7\u6ee4","text":"<p>Filtering \u89e3\u51b3\u4e86 <code>Can we detect inevitable failure early?</code> \u7684\u95ee\u9898</p> <p>Forward Checking</p> <p>\u68c0\u67e5<code>\u672a\u5206\u914d\u53d8\u91cf</code>\u4e0e<code>\u5f53\u524d\u5206\u914d\u53d8\u91cf</code>\u7684\u76f4\u63a5\u51b2\u7a81\uff01\uff08\u4f46\u5176\u4ed6\u51b2\u7a81\u4ed6\u5c31\u65e0\u80fd\u4e3a\u529b\u4e86\uff09</p> <p></p> <p>Arc Consistency</p> <ul> <li>Important: If X loses a value, neighbors of X need to be rechecked!</li> <li>Arc consistency detects failure earlier than forward checking</li> <li>Can be run as a preprocessor or after each assignment</li> <li>What's the downside of enforcing arc consistency?</li> <li>Remember: Delete from the tail!</li> </ul> <p><pre><code>function AC-3(csp) returns the CSP, possibly with reduced domains\n  inputs: csp, a binary CSP with variables {X_1, X_2, ..., X_n}\n  local variables: queue, a queue of arcs, initially all the arcs in csp\n\n  while queue is not empty do\n    (X_i, X_j) &lt;- REMOVE-FIRST(queue)\n    if REMOVE-INCONSISTENT-VALUES(X_i, X_j) then\n      for each X_k in NEIGHBORS[X_i] do\n        add (X_k, X_i) to queue\n\nfunction REMOVE-INCONSISTENT-VALUES(X_i, X_j) returns true iff succeeds\n  removed &lt;- false\n  for each x in DOMAIN[X_i] do\n    if no value y in DOMAIN[X_j] allows (x, y) to satisfy the constraint X_i &lt;-&gt; X_j\n    then delete x from DOMAIN[X_i]; removed &lt;- true\n  return removed\n</code></pre> Runtime: \\(O(n^2d^3)\\), can be reduced to \\(O(n^2d^2)\\) BUT detecting all possible futrue problems is NP-hard.</p> <p>Limitation of Arc Consistancy See an example below:  Arc Consistancy satisfied!! BUT no solution!!!</p>"},{"location":"Lecture%2003%20CSP/#32-ordering","title":"3.2. Ordering \u6392\u5e8f","text":"<p>\u5728\u6c42\u89e3 CSP \u65f6\uff0c\u6211\u4eec\u4f1a\u5bf9\u6240\u6d89\u53ca\u7684\u53d8\u91cf\u548c\u503c\u8fdb\u884c\u4e00\u4e9b\u6392\u5e8f\u3002 \u5728\u5b9e\u8df5\u4e2d\uff0c\u4f7f\u7528\u4e24\u4e2a\u5e7f\u6cdb\u7684\u539f\u5219\u201c\u52a8\u6001\u201d\u8ba1\u7b97\u4e0b\u4e00\u4e2a\u53d8\u91cf\u53ca\u5176\u5bf9\u5e94\u7684\u503c\u901a\u5e38\u66f4\u6709\u6548\uff0c\u8fd9\u4e24\u4e2a\u539f\u5219\u662f\uff1a\u6700\u5c0f\u5269\u4f59\u503c\u548c\u6700\u5c0f\u7ea6\u675f\u503c\uff1a - Minimum Remaining Value \u6700\u5c0f\u5269\u4f59\u503c (MRV)   \u9009\u62e9\u672a\u8d4b\u503c\u4e14\u5269\u4f59\u6709\u6548\u503c\u6700\u5c11\u7684\u53d8\u91cf\uff08\u5373\u7ea6\u675f\u6700\u4e25\u683c\u7684\u53d8\u91cf\uff09\u3002</p> <ul> <li>Least Constraining Value \u6700\u5c0f\u7ea6\u675f\u503c (LCV)   \u9009\u62e9\u90a3\u4e2a\u5bf9\u5176\u4ed6\uff08\u90bb\u5c45\uff09\u53d8\u91cf\u7ea6\u675f\u6700\u5c11\u7684\u503c\uff0c\u5373\u9009\u62e9\u4e00\u4e2a\u80fd\u4e3a\u672a\u6765\u7684\u9009\u62e9\u4fdd\u7559\u6700\u591a\u53ef\u80fd\u6027\u7684\u503c\u3002   \u8fd9\u9700\u8981\u989d\u5916\u7684\u8ba1\u7b97\uff08\u4f8b\u5982\uff0c\u5bf9\u6bcf\u4e2a\u503c\u91cd\u65b0\u8fd0\u884c\u5f27\u4e00\u81f4\u6027/\u524d\u5411\u68c0\u67e5\u6216\u5176\u4ed6\u8fc7\u6ee4\u65b9\u6cd5\u4ee5\u627e\u5230 LCV\uff09\uff0c\u4f46\u4ecd\u7136\u53ef\u4ee5\u6839\u636e\u4f7f\u7528\u60c5\u51b5\u63d0\u9ad8\u901f\u5ea6\u3002</li> </ul> <p>\u8fd9\u5f88\u6709\u610f\u601d\uff01\u5bf9\u4e8evariable\u6211\u4eec\u9009\u62e9\u505a\u6700\u4e25\u683c\u7684\uff0c\u4f46\u5bf9\u4e8evalue\u6211\u4eec\u9009\u62e9\u6700\u5bbd\u677e\u7684\u3002</p>"},{"location":"Lecture%2004%20CSP%20II/","title":"Lecture 04 CPS II","text":""},{"location":"Lecture%2004%20CSP%20II/#cs-180-introduction-to-ai","title":"CS 180 - Introduction to AI","text":""},{"location":"Lecture%2004%20CSP%20II/#lecture-4-constraint-satisfaction-problems-ii","title":"Lecture 4: Constraint Satisfaction Problems II","text":""},{"location":"Lecture%2004%20CSP%20II/#recall","title":"Recall","text":"<p>1. K-Consistency</p> <p>Increasing degrees of consistency</p> <ul> <li> <p>1-Consistency (Node Consistency): Each single node's domain has a value which meets that node's unary constraints.</p> </li> <li> <p>2-Consistency (Arc Consistency): For each pair of nodes, any consistent assignment to one can be extended to the other.</p> </li> <li> <p>K-Consistency: For each \\(k\\) nodes, any consistent assignment to \\(k-1\\) can be extended to the \\(k^{th}\\) node.</p> </li> </ul> <p>Higher \\(k\\) more expensive to compute (but makes filtering better: tradeoff!).</p> <p>2. Strong K-Consistency * Strong k-consistency\u7684\u533a\u522b: also k-1, k-2, ... 1 consistent.</p> <ul> <li> <p>Claim: strong n-consistency means we can solve without backtracking! Problem: NP-hard Problem, so it's extremely hard to build a Strong K-Consistency!!!</p> </li> <li> <p>Lots of middle ground between arc consistency and n-consistency! (e.g. k=3, called path consistency)</p> </li> </ul>"},{"location":"Lecture%2004%20CSP%20II/#1-structure-improvement-to-solving-csp-continue","title":"1. Structure: Improvement to solving CSP (continue)","text":"<p>Theorem: if the constraint graph has no loops, the CSP can be solved in \\(O(n \\cdot d^2)\\) time. \u5bf9\u4e8e\u4e00\u822c\u7684CSP\u95ee\u9898 worst-case time is \\(O(d^n)\\).</p> <p>Tree Structured CSPs </p> <p>\u6b65\u9aa4\uff1a     1. \u9996\u5148\uff0c\u5728\u7ea6\u675f\u56fe\u4e2d\u4e3a CSP \u9009\u62e9\u4e00\u4e2a\u4efb\u610f\u8282\u70b9\u4f5c\u4e3a\u6811\u7684\u6839\uff08\u54ea\u4e00\u4e2a\u5e76\u4e0d\u91cd\u8981\uff0c\u56e0\u4e3a\u57fa\u672c\u56fe\u8bba\u544a\u8bc9\u6211\u4eec\u6811\u7684\u4efb\u4f55\u8282\u70b9\u90fd\u53ef\u4ee5\u4f5c\u4e3a\u6839\uff09\u3002     2. \u5c06\u6811\u4e2d\u6240\u6709\u65e0\u5411\u8fb9\u8f6c\u6362\u4e3a\u6307\u5411\u8fdc\u79bb\u6839\u8282\u70b9\u7684\u6709\u5411\u8fb9\u3002\u7136\u540e\u5bf9\u751f\u6210\u7684\u6709\u5411\u65e0\u73af\u56fe\u8fdb\u884c\u6392\u5e8f\uff0c\u4f7f\u6240\u6709\u8fb9\u90fd\u6307\u5411\u53f3\u65b9\u3002     3. \u6267\u884c\u5f27\u4e00\u81f4\u6027\u7684\u53cd\u5411\u4f20\u9012\uff1a\u4ece \\(i = n\\) \u5411\u4e0b\u8fed\u4ee3\u5230 \\(i = 2\\) \uff0c\u5bf9\u6240\u6709\u5f27 \\(Parent(X_i) \\to X_i\\) \u5f3a\u5236\u6267\u884c\u5f27\u4e00\u81f4\u6027\u3002     4. \u6700\u540e\uff0c\u6267\u884c\u524d\u5411\u8d4b\u503c\u3002\u4ece \\(X_1\\) \u5f00\u59cb\uff0c\u5230 \\(X_n\\) \uff0c\u4e3a\u6bcf\u4e2a \\(X_i\\) \u8d4b\u503c\uff0c\u4f7f\u5176\u4e0e\u5176\u7236\u8282\u70b9\u7684\u503c\u4e00\u81f4\u3002</p> <p>\u7531\u4e8e\u6211\u4eec\u5728\u6240\u6709\u8fd9\u4e9b\u5f27\u4e0a\u90fd\u5f3a\u5236\u4e86\u5f27\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u65e0\u8bba\u6211\u4eec\u4e3a\u4efb\u4f55\u8282\u70b9\u9009\u62e9\u4ec0\u4e48\u503c\uff0c\u6211\u4eec\u90fd\u77e5\u9053\u5176\u5b50\u8282\u70b9\u81f3\u5c11\u4f1a\u6709\u4e00\u4e2a\u4e00\u81f4\u7684\u503c\u3002</p>"},{"location":"Lecture%2004%20CSP%20II/#2-improving-structure","title":"2. Improving Structure","text":"<p>Tree is not that common! -&gt; Nearly Tree-Structured CSPs</p> <p></p> <ul> <li> <p>Conditioning (\u6761\u4ef6\u5316): \u5b9e\u4f8b\u5316\u4e00\u4e2a\u53d8\u91cf\uff0c\u4fee\u526a\u5176\u90bb\u5c45\u7684\u57df\u3002</p> </li> <li> <p>Cutset Conditioning (\u5272\u96c6\u6761\u4ef6\u5316): \u5b9e\u4f8b\u5316\uff08\u901a\u8fc7\u6240\u6709\u65b9\u5f0f\uff09\u4e00\u7ec4\u53d8\u91cf\uff0c\u4f7f\u5f97\u5269\u4f59\u7684\u7ea6\u675f\u56fe\u6210\u4e3a\u4e00\u68f5\u6811\u3002</p> </li> <li>\u5272\u96c6\u5927\u5c0f c \u7684\u8fd0\u884c\u65f6\u590d\u6742\u5ea6\u4e3a \\(O((d^c)(n-c)d^2)\\)\uff0c\u5f53 \\(c\\) \u5f88\u5c0f\u65f6\uff0c\u8fd0\u884c\u901f\u5ea6\u975e\u5e38\u5feb\u3002</li> <li></li> </ul>"},{"location":"Lecture%2004%20CSP%20II/#3-local-search","title":"3. Local Search \u5c40\u90e8\u641c\u7d22","text":"<p>Backtrcking search is not the only algorithm that exists for solving CSP.</p> <p>Another widely used algorithm is Local Search.</p> <p>\u5c40\u90e8\u641c\u7d22\u901a\u8fc7\u8fed\u4ee3\u6539\u8fdb\u6765\u5de5\u4f5c\u2014\u2014\u9996\u5148\u5bf9\u503c\u8fdb\u884c\u968f\u673a\u8d4b\u503c\uff0c\u7136\u540e\u8fed\u4ee3\u5730\u9009\u62e9\u4e00\u4e2a\u968f\u673a\u51b2\u7a81\u53d8\u91cf\uff0c\u5e76\u5c06\u5176\u503c\u91cd\u65b0\u8d4b\u7ed9\u8fdd\u53cd\u7ea6\u675f\u6700\u5c11\u7684\u53d8\u91cf\uff0c\u76f4\u5230\u4e0d\u518d\u5b58\u5728\u7ea6\u675f\u8fdd\u53cd\u3002(min-conflicts heuristic)</p> <p>Local Search \u4e0d\u5b8c\u6574\u4e5f\u4e0d\u6700\u4f18\u3002</p> <p>Critical ratio \u4e34\u754c\u6bd4\u7387\uff1a\u8fd9\u4e2a\u6bd4\u7387\u9644\u8fd1\uff0c\u6210\u672c\u4f1a\u6781\u5176\u6602\u8d35\u3002 $$ R = \\frac{\\text{number of constraints}}{\\text{number of cariables}} $$</p> <p>\u4e09\u79cd\u7b97\u6cd5\uff08\u4f9b\u4e86\u89e3\uff09\uff1ahill-climbing, simulated annealing, and genetic algorithms</p>"},{"location":"Lecture%2005%20Game%20Tree/","title":"Lecture 05 Game Tree","text":""},{"location":"Lecture%2005%20Game%20Tree/#cs-180-introduction-to-ai","title":"CS 180 - Introduction to AI","text":""},{"location":"Lecture%2005%20Game%20Tree/#lecture-5-game-tree-adversarial-search","title":"Lecture 5: Game Tree / Adversarial Search","text":""},{"location":"Lecture%2005%20Game%20Tree/#1-deterministic-games","title":"1. Deterministic Games","text":"<p>Many possible formalizations, one is:</p> <ul> <li>Initial state: <code>s_0</code></li> <li>Players: <code>Players(s)</code> denote whose turn it is</li> <li>Actions: <code>Actions(s)</code> available actions for the player</li> <li>Transition model: <code>Result(s, a)</code></li> <li>Terminal test (True/False): <code>Terminal-test(s)</code></li> <li>Terminal values: <code>Utility(s, player)</code></li> </ul>"},{"location":"Lecture%2005%20Game%20Tree/#2-settings-for-zero-sum-game","title":"2. Settings for Zero-Sum Game","text":"<p>\u5047\u8bbe\u5403\u8c46\u4eba\u521d\u59cb\u5f97\u5206\u4e3a 10 \u5206\uff0c\u6bcf\u8d70\u4e00\u6b65\u6263 1 \u5206\uff0c\u76f4\u5230\u5403\u6389\u5c0f\u7403\u4e3a\u6b62\u3002\u6b64\u65f6\u6e38\u620f\u5230\u8fbe\u7ec8\u6b62\u72b6\u6001\u5e76\u7ed3\u675f\u3002 </p> <p>\u72b6\u6001\u503c\u5b9a\u4e49\u4e3a\u4ee3\u7406\u5728\u8be5\u72b6\u6001\u4e0b\u80fd\u591f\u5b9e\u73b0\u7684\u6700\u4f73\u7ed3\u679c\uff08Utility\uff09,\u975e\u7ec8\u7aef\u72b6\u6001\u7684\u503c\u5b9a\u4e49\u4e3a\u5176\u5b50\u72b6\u6001\u503c\u7684\u6700\u5927\u503c\u3002</p> <p>\u62d3\u5c55\u5230\u4e24\u4e2aagent\uff1a </p>"},{"location":"Lecture%2005%20Game%20Tree/#3-minimax","title":"3. Minimax","text":"<p>\u57fa\u4e8e\u4e00\u4e2a\u6fc0\u52b1\u5047\u8bbe\uff1a\u6211\u4eec\u9762\u5bf9\u7684\u5bf9\u624b\u4f1a\u91c7\u53d6\u6700\u4f18\u884c\u4e3a\uff0c\u5e76\u4e14\u603b\u662f\u4f1a\u505a\u51fa\u5bf9\u6211\u4eec\u6700\u4e0d\u5229\u7684\u4e3e\u52a8\u3002 \u5206\u6790\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff1a </p> <p>\\(\\forall\\)agent-controlled states, \\(V(s) = \\max_{s' \\in successors(s)} V(s')\\)</p> <p>\\(\\forall\\)opponent-controlled states, \\(V(s) = \\min_{s' \\in successors(s)} V(s')\\)</p> <p></p>"},{"location":"Lecture%2006%20Game%20Tree%20II/","title":"Lecture 06 Game Tree II","text":""},{"location":"Lecture%2006%20Game%20Tree%20II/#cs-180-introduction-to-ai","title":"CS 180 - Introduction to AI","text":""},{"location":"Lecture%2006%20Game%20Tree%20II/#lecture-6-game-tree-ii","title":"Lecture 6: Game Tree II","text":""},{"location":"Lecture%2006%20Game%20Tree%20II/#1-minimax-improving","title":"1. Minimax Improving","text":""},{"location":"Lecture%2006%20Game%20Tree%20II/#11-alpha-beta-pruning","title":"1.1 Alpha-Beta Pruning","text":"<p>Intuion: Minimax\u770b\u7740\u4e0d\u9519\uff0c\u4f46\u4ed6\u4e0eDFS\u5f88\u7c7b\u4f3c\u2014\u2014\u65f6\u95f4\u590d\u6742\u5ea6\\(O(b^m)\\)\u2014\u2014\u8fd0\u884c\u65f6\u95f4\u592a\u957f\uff01</p> <p>\u4f18\u5316\u65b9\u6cd5\uff1a\\(\\alpha-\\beta\\) pruning\u3002</p> <p></p> <p>\u53ea\u8981\u6211\u4eec\u8bbf\u95ee\u4e86\u4e2d\u95f4\u6700\u5c0f\u503c\u503c\u4e3a 2 \u7684\u5b50\u8282\u70b9\uff0c\u5c31\u4e0d\u518d\u9700\u8981\u67e5\u770b\u4e2d\u95f4\u6700\u5c0f\u503c\u7684\u5176\u4ed6\u5b50\u8282\u70b9\u4e86\uff01 \u4e3a\u4ec0\u4e48\u5462\uff1f\u56e0\u4e3a\u6211\u4eec\u5df2\u7ecf\u770b\u5230\u4e2d\u95f4\u6700\u5c0f\u503c\u7684\u4e00\u4e2a\u5b50\u8282\u70b9\u7684\u503c\u4e3a 2\uff0c\u6240\u4ee5\u6211\u4eec\u77e5\u9053\uff0c\u65e0\u8bba\u5176\u4ed6\u5b50\u8282\u70b9\u7684\u503c\u662f\u4ec0\u4e48\uff0c\u4e2d\u95f4\u6700\u5c0f\u503c\u7684\u503c\u6700\u591a\u4e3a 2\u3002 \u65e2\u7136\u8fd9\u4e00\u70b9\u5df2\u7ecf\u786e\u5b9a\uff0c\u8ba9\u6211\u4eec\u518d\u8fdb\u4e00\u6b65\u601d\u8003\u2014\u2014\u6839\u8282\u70b9\u7684\u6700\u5927\u503c\u8282\u70b9\u6b63\u5728\u5de6\u8fb9\u6700\u5c0f\u503c\u7684\u503c 3 \u548c \u2264 2 \u4e4b\u95f4\u505a\u51fa\u9009\u62e9\uff0c\u5b83\u80af\u5b9a\u4f1a\u4f18\u5148\u9009\u62e9\u5de6\u8fb9\u6700\u5c0f\u503c\u8fd4\u56de\u7684 3\uff0c\u800c\u4e0d\u662f\u4e2d\u95f4\u6700\u5c0f\u503c\u8fd4\u56de\u7684\u503c\uff0c\u800c\u4e0d\u7ba1\u5176\u5269\u4f59\u5b50\u8282\u70b9\u7684\u503c\u662f\u4ec0\u4e48\u3002</p> <p></p> <p>Alpha (\\(\u03b1\\)): MAX \u73a9\u5bb6\u7684\u201c\u6700\u4f4e\u627f\u8bfa\u201d\uff08\u5728\u5df2\u63a2\u7d22\u8fc7\u7684\u8def\u5f84\u4e2d\uff0cMAX \u73a9\u5bb6\u81f3\u5c11\u80fd\u83b7\u5f97\u7684\u5206\u6570\u3002\uff09 Beta (\\(\u03b2\\)): MIN \u73a9\u5bb6\u7684\u201c\u6700\u9ad8\u9650\u5236\u201d</p> \\[\\text{if } \u03b1 \u2265 \u03b2 \\rightarrow pruning\\]"},{"location":"Lecture%2006%20Game%20Tree%20II/#12-evaluation-function","title":"1.2. Evaluation Function","text":"<p>depth-limited minimax\uff1a\u8bbe\u7f6e\u6df1\u5ea6\u9650\u5236 \\(\\rightarrow\\) \u6700\u540e\u4e00\u5c42\u7684\u72b6\u6001\u503c\u600e\u4e48\u5f97\u5230\uff1f Heuristics\uff01  In this case we call it evaluation function!</p> <p>\\(Eval(s) = w_1 f_1(s) + w_2 f_2(s) + \\dots + w_n f_n(s)\\)</p>"},{"location":"Lecture%2006%20Game%20Tree%20II/#2-expectimax","title":"2. Expectimax","text":"<p>\u7531\u4e8e\u6781\u5c0f\u6781\u5927\u7b97\u6cd5\u8ba4\u4e3a\u5b83\u6b63\u5728\u5bf9\u6700\u4f18\u5bf9\u624b\u505a\u51fa\u54cd\u5e94\uff0c\u56e0\u6b64\u5728\u65e0\u6cd5\u4fdd\u8bc1\u5bf9\u667a\u80fd\u4f53\u52a8\u4f5c\u505a\u51fa\u6700\u4f18\u54cd\u5e94\u7684\u60c5\u51b5\u4e0b\uff0c\u5b83\u5f80\u5f80\u4f1a\u8fc7\u4e8e\u60b2\u89c2\u3002</p> <p>\u8fd9\u79cd\u968f\u673a\u6027\u53ef\u4ee5\u901a\u8fc7\u6781\u5c0f\u6781\u5927\u503c\uff08minimax\uff09\u7684\u6cdb\u5316\uff08expectimax\uff09\u6765\u8868\u793a\u3002expectimax \u5c06\u673a\u4f1a\u8282\u70b9\u5f15\u5165\u535a\u5f08\u6811\uff0c\u5b83\u4e0d\u50cf\u6700\u5c0f\u5316\u5668\u8282\u70b9\u90a3\u6837\u8003\u8651\u6700\u574f\u60c5\u51b5\uff0c\u800c\u662f\u8003\u8651\u5e73\u5747\u60c5\u51b5\u3002</p> <p>\u8282\u70b9\u503c\u89c4\u5219\u5982\u4e0b\uff1a \\(\\(\\forall\\text{agent-controlled states, } V(s) = \\max_{s' \\in successors(s)} V(s')\\)\\)</p> \\[\\forall\\text{chance states, } V(s) = \\sum_{s' \\in successors(s)} p(s'|s)V(s')\\] \\[\\forall\\text{terminal states, } V(s) = \\text{known}\\] <p>\u5728\u4e0a\u9762\u7684\u516c\u5f0f\u4e2d\uff0c\\(p(s'|s)\\)\u6307\u7684\u662f\u7ed9\u5b9a\u7684\u4e0d\u786e\u5b9a\u6027\u52a8\u4f5c\u5bfc\u81f4\u4ece\u72b6\u6001 \\(s\\) \u79fb\u52a8\u5230 \\(s'\\) \u7684\u6982\u7387\uff0c\u6216\u8005\u662f\u5bf9\u4e8e\u9009\u62e9\u5bfc\u81f4\u4ece\u72b6\u6001 \\(s\\) \u79fb\u52a8\u5230 \\(s'\\) \u7684\u52a8\u4f5c\u7684\u6982\u7387\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u6e38\u620f\u7684\u5177\u4f53\u60c5\u51b5\u548c\u6240\u8003\u8651\u7684\u6e38\u620f\u6811\u3002</p> <p>\u4ece\u8fd9\u4e2a\u5b9a\u4e49\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u51fa minimax \u53ea\u662f expectimax \u7684\u4e00\u4e2a\u7279\u4f8b\u3002</p> <p></p>"},{"location":"Lecture%2006%20Game%20Tree%20II/#3-other-game","title":"3. Other Game","text":"<p>\u4e0d\u540c\u7684\u667a\u80fd\u4f53\u5728\u535a\u5f08\u4e2d\u53ef\u80fd\u627f\u62c5\u7740\u4e0d\u540c\u7684\u4efb\u52a1\uff0c\u800c\u8fd9\u4e9b\u4efb\u52a1\u5e76\u4e0d\u76f4\u63a5\u6d89\u53ca\u5f7c\u6b64\u4e4b\u95f4\u7684\u4e25\u683c\u7ade\u4e89\u3002</p> <p>\u8fd9\u7c7b\u535a\u5f08\u53ef\u4ee5\u7528\u4ee5\u591a\u667a\u80fd\u4f53\u6548\u7528\u4e3a\u7279\u5f81\u7684\u6811\u6765\u6784\u5efa\u2014\u2014\u8868\u793a\u4e3a\u5143\u7ec4\uff0c\u5143\u7ec4\u4e2d\u7684\u4e0d\u540c\u503c\u5bf9\u5e94\u4e8e\u4e0d\u540c\u667a\u80fd\u4f53\u7684\u72ec\u7279\u6548\u7528\u3002\u7136\u540e\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u5c1d\u8bd5\u5728\u5176\u63a7\u5236\u7684\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6700\u5927\u5316\u81ea\u8eab\u7684\u6548\u7528\u3002</p>"},{"location":"Lecture%2007%2608%20MDP/","title":"Lecture 07&08 MDP","text":""},{"location":"Lecture%2007%2608%20MDP/#cs-180-introduction-to-ai","title":"CS 180 - Introduction to AI","text":""},{"location":"Lecture%2007%2608%20MDP/#lecture-78-markov-decision-processes","title":"Lecture 7&amp;8: Markov Decision Processes","text":""},{"location":"Lecture%2007%2608%20MDP/#1-non-deterministic-search","title":"1. Non-deterministic Search","text":"<p>An MDP is defined by: * A set of states \\(s \\in S\\) * A set of actions \\(a \\in A\\) * A transition function \\(T(s, a, s')\\) \u5f15\u5165\u4e86\u975e\u786e\u5b9a\u6027\u52a8\u4f5c\u7684\u53ef\u80fd\u6027     * Probability that \\(a\\) from \\(s\\) leads to \\(s'\\), i.e., \\(P(s'|s, a)\\)     * Also called the model or the dynamics * A reward function \\(R(s, a, s')\\)</p> <ul> <li>A start state</li> <li>Maybe a terminal state</li> </ul> <p>Discount factor \\(\\gamma\\) Sooner rewards probably have higher utility than later rewards! $$ \\gamma \\in [0,1] $$</p> <p>Markovianess: \u65e0\u8bb0\u5fc6\u6027\uff0c \u5982\u679c\u6211\u4eec\u77e5\u9053\u5f53\u524d\u72b6\u6001\uff0c\u90a3\u4e48\u4e86\u89e3\u8fc7\u53bb\u5e76\u4e0d\u80fd\u7ed9\u6211\u4eec\u63d0\u4f9b\u66f4\u591a\u5173\u4e8e\u672a\u6765\u7684\u4fe1\u606f\u3002</p>"},{"location":"Lecture%2007%2608%20MDP/#2-bellman-optimality-equation","title":"2. Bellman Optimality Equation","text":"<p>\u9996\u5148\u6211\u4eec\u8981\u4e3a\u72b6\u6001\u5b9a\u4ef7\uff1a\u201c\u5f53\u524d\u201d\u72b6\u6001\u7684\u4ef7\u503c\uff0c\u662f\u7531\u201c\u672a\u6765\u201d\u72b6\u6001\u7684\u4ef7\u503c\u6765\u5b9a\u4e49\u7684\u3002 \\(\\(\\text{Bellman equation: \\\\   \\  } V^*(s) = \\max_a Q^*(s, a)\\)\\) \u542b\u4e49\uff1a \u4e00\u4e2a\u72b6\u6001 \\(s\\) \u7684\u6700\u4f18\u4ef7\u503c \u7b49\u4e8e\u4ece\u8fd9\u4e2a\u72b6\u6001\u51fa\u53d1\uff0c\u6240\u80fd\u91c7\u53d6\u7684\u6240\u6709\u52a8\u4f5c \\(a\\) \u4e2d\uff0c\u80fd\u4ea7\u751f\u7684\u90a3\u4e2a\u6700\u4f18\u7684\u52a8\u4f5c\u4ef7\u503c (\\(Q^*(s, a)\\))\u3002 \\(\\(Q^*(s, a) = \\sum_{s'} T(s, a, s')[R(s, a, s') + \\gamma V^*(s')]\\)\\)</p>"},{"location":"Lecture%2007%2608%20MDP/#3-value-iteration","title":"3. Value Iteration","text":"<p>\u5982\u4f55\u5b9e\u9645\u8ba1\u7b97\u8fd9\u4e9b\u6700\u4f18\u503c\uff1f \u6211\u4eec\u9700\u8981time limited values!</p> <p>\u5bf9\u4e8e\u65f6\u95f4\u9650\u5236\u4e3a \\(k\\) \u4e2a\u65f6\u95f4\u6b65\u7684\u72b6\u6001 \\(s\\) \uff0c\u65f6\u95f4\u9650\u5236\u503c\u8868\u793a\u4e3a \\(V_k(s)\\) \uff0c\u5b83\u8868\u793a\u5728\u6240\u8003\u8651\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7ec8\u6b62\u4e8e \\(k\\) \u4e2a\u65f6\u95f4\u6b65\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece \\(s\\) \u53ef\u83b7\u5f97\u7684\u6700\u5927\u9884\u671f\u6548\u7528\u3002</p> <p>\u7b49\u6548\u5730\uff0c\u8fd9\u4e5f\u662f\u5728 MDP \u7684\u641c\u7d22\u6811\u4e0a\u8fd0\u884c\u6df1\u5ea6\u4e3a \\(k\\) \u7684 expectimax \u51fd\u6570\u6240\u8fd4\u56de\u7684\u7ed3\u679c\u3002</p> <p>\u8fd0\u884c\u65b9\u5f0f\u5982\u4e0b\uff1a</p> <ol> <li>\\(\\forall s \\in S\\)\uff0c\u521d\u59cb\u5316 \\(V_0(s) = 0\\)\u3002\u8fd9\u662f\u5f88\u76f4\u89c2\u7684\uff0c\u56e0\u4e3a\u8bbe\u7f6e\u4e00\u4e2a 0 \u6b65\u7684\u65f6\u9650\u610f\u5473\u7740\u5728\u7ec8\u6b62\u524d\u4e0d\u80fd\u91c7\u53d6\u4efb\u4f55\u884c\u52a8\uff0c\u56e0\u6b64\u4e5f\u65e0\u6cd5\u83b7\u5f97\u4efb\u4f55\u5956\u52b1\u3002</li> <li>\u91cd\u590d\u4ee5\u4e0b\u66f4\u65b0\u89c4\u5219\u76f4\u5230\u6536\u655b\uff1a     \\(\\(\\forall s \\in S, V_{k+1}(s) \\leftarrow \\max_a \\sum_{s'} T(s, a, s')[R(s, a, s') + \\gamma V_k(s')]\\)\\)</li> </ol> <p>\u5728\u4ef7\u503c\u8fed\u4ee3\u7684\u7b2c \\(k\\) \u6b21\u8fed\u4ee3\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u6bcf\u4e2a\u72b6\u6001\u7684\u65f6\u9650\u4e3a \\(k\\) \u7684\u503c\u6765\u751f\u6210\u65f6\u9650\u4e3a \\((k+1)\\) \u7684\u503c\u3002\u672c\u8d28\u4e0a\uff0c\u6211\u4eec\u4f7f\u7528\u5df2\u8ba1\u7b97\u7684\u5b50\u95ee\u9898\u89e3\uff08\u6240\u6709\u7684 \\(V_k(s)\\)\uff09\u6765\u8fed\u4ee3\u5730\u6784\u5efa\u66f4\u5927\u5b50\u95ee\u9898\u7684\u89e3\uff08\u6240\u6709\u7684 \\(V_{k+1}(s)\\)\uff09\uff1b\u8fd9\u5c31\u662f\u4ef7\u503c\u8fed\u4ee3\u6210\u4e3a\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u7684\u539f\u56e0\u3002</p> <p>Policy Extraction</p> <p>\u539f\u7406\uff1a\u5982\u679c\u4f60\u5904\u4e8e\u72b6\u6001 \\(s\\) \uff0c\u4f60\u5e94\u8be5\u91c7\u53d6\u52a8\u4f5c \\(a\\) \uff0c\u4ee5\u83b7\u5f97\u6700\u5927\u7684\u9884\u671f\u6548\u7528\u3002 $$ \\forall s \\in \\mathcal{S}, \\pi^(s) = \\underset{a}{\\text{argmax}} Q^(s, a) = \\underset{a}{\\text{argmax}} \\sum_{s'} T(s, a, s')[R(s, a, s') + \\gamma V^*(s')] $$</p> <p>Q-Value Iteration</p> <p>Q \u503c\u8fed\u4ee3\u662f\u4e00\u79cd\u8ba1\u7b97\u65f6\u95f4\u53d7\u9650\u7684 Q \u503c\u7684\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u2014\u2014\u53ea\u80fd\u73a9 N \u6b65\uff08\u6bd4\u598210\u6b65\uff09\uff0c\u6e38\u620f\u5fc5\u5b9a\u7ed3\u675f\u3002\u76ee\u6807\u662f\u5728\u8fd9\u6709\u9650\u7684 N \u6b65\u5185\uff0c\u8ba9\u603b\u6536\u76ca\u6700\u5927\u5316\u3002 $$ Q_{k+1}(s, a) \\leftarrow \\sum_{s'} T(s, a, s')[R(s, a, s') + \\gamma \\max_{a'} Q_k(s', a')] $$</p>"},{"location":"Lecture%2007%2608%20MDP/#4-policy-iteration","title":"4. Policy Iteration","text":"<p>\u6d41\u7a0b\uff1a 1. \u5b9a\u4e49\u4e00\u4e2a\u521d\u59cb\u7b56\u7565\u3002\u8fd9\u4e2a\u7b56\u7565\u53ef\u4ee5\u662f\u4efb\u610f\u7684\uff08\u4f46\u662f\u5982\u679c\u521d\u59cb\u7b56\u7565\u66f4\u63a5\u8fd1\u6700\u4f18\u7b56\u7565\uff0c\u6536\u655b\u4f1a\u66f4\u5feb\uff09\u3002 2. \u91cd\u590d\u4ee5\u4e0b\u6b65\u9aa4\u76f4\u5230\u6536\u655b\uff1a     * \u901a\u8fc7 \u7b56\u7565\u8bc4\u4f30\uff08policy evaluation\uff09\u6765\u8bc4\u4f30\u5f53\u524d\u7b56\u7565\u3002\u5bf9\u4e8e\u4e00\u4e2a\u7b56\u7565 \\(\\pi\\)\uff0c\u7b56\u7565\u8bc4\u4f30\u610f\u5473\u7740\u8ba1\u7b97\u6240\u6709\u72b6\u6001 \\(s\\) \u7684 \\(V^\\pi(s)\\)\uff0c\u5176\u4e2d \\(V^\\pi(s)\\) \u662f\u6307\u5728\u72b6\u6001 \\(s\\) \u5f00\u59cb\u5e76\u9075\u5faa \\(\\pi\\) \u65f6\u6240\u671f\u671b\u7684\u6548\u7528\uff1a     \\(\\(V^\\pi(s) = \\sum_{s'} T(s, \\pi(s), s')[R(s, \\pi(s), s') + \\gamma V^\\pi(s')]\\)\\)     \u7531\u4e8e\u6211\u4eec\u4e3a\u6bcf\u4e2a\u72b6\u6001\u56fa\u5b9a\u4e86\u4e00\u4e2a\u5355\u4e00\u7684\u884c\u52a8\uff0c\u6211\u4eec\u4e0d\u518d\u9700\u8981 \\(\\max\\) \u8fd0\u7b97\u7b26\u3002     \u6216\u8005\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u66f4\u65b0\u89c4\u5219\uff0c\u50cf\u4ef7\u503c\u8fed\u4ee3\uff08value iteration\uff09\u4e00\u6837\uff0c\u8fed\u4ee3\u8ba1\u7b97 \\(V^{\\pi_i}(s)\\) \u76f4\u5230\u6536\u655b\uff1a     \\(\\(V_{k+1}^{\\pi_i}(s) \\leftarrow \\sum_{s'} T(s, \\pi_i(s), s')[R(s, \\pi_i(s), s') + \\gamma V_k^{\\pi_i}(s')]\\)\\)     \u7136\u800c\uff0c\u8fd9\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u901a\u5e38\u8f83\u6162\u3002     * \u4e00\u65e6\u6211\u4eec\u8bc4\u4f30\u4e86\u5f53\u524d\u7b56\u7565\uff0c\u5c31\u4f7f\u7528\u7b56\u7565\u6539\u8fdb\uff08policy improvement\uff09\u6765\u751f\u6210\u4e00\u4e2a\u66f4\u597d\u7684\u7b56\u7565\u3002     \u7b56\u7565\u6539\u8fdb\u4f7f\u7528\u5bf9\u7b56\u7565\u8bc4\u4f30\u4ea7\u751f\u7684\u72b6\u6001\u4ef7\u503c\uff08values of states\uff09\u8fdb\u884c\u7b56\u7565\u63d0\u53d6\uff0c\u4ece\u800c\u751f\u6210\u8fd9\u4e2a\u65b0\u7684\u3001\u6539\u8fdb\u540e\u7684\u7b56\u7565\uff1a     \\(\\(\\pi_{i+1}(s) = \\underset{a}{\\text{argmax}} \\sum_{s'} T(s, a, s')[R(s, a, s') + \\gamma V^{\\pi_i}(s')]\\)\\) \u5982\u679c \\(\\pi_{i+1} = \\pi_i\\)\uff0c\u5219\u7b97\u6cd5\u5df2\u7ecf\u6536\u655b\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u51fa\u7ed3\u8bba\uff1a\\(\\pi_{i+1} =\\pi_i = \\pi^*\\)</p>"},{"location":"Lecture%2009%2610%20RL/","title":"Lecture 09&10 RL","text":""},{"location":"Lecture%2009%2610%20RL/#cs-180-introduction-to-ai","title":"CS 180 - Introduction to AI","text":""},{"location":"Lecture%2009%2610%20RL/#lecture-910-reinforcement-learning","title":"Lecture 9&amp;10: Reinforcement Learning","text":"<p>\u4e0a\u4e00\u8282\u8bfe\uff1a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u7528\u8bf8\u5982 Value Iteration \u548c Policy Iteration \u7b49\u6280\u672f\u6765\u6c42\u89e3\u8be5\u8fc7\u7a0b\uff0c\u4ee5\u8ba1\u7b97\u72b6\u6001\u7684\u6700\u4f18\u503c\u5e76\u63d0\u53d6\u6700\u4f18\u7b56\u7565\u3002</p> <p>\u6c42\u89e3\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u662f \u79bb\u7ebf\u89c4\u5212 (offline planning) \u7684\u4e00\u4e2a\u4f8b\u5b50\uff0c\u5176\u4e2d Agent \u5bf9\u8f6c\u6362\u51fd\u6570\u548c\u5956\u52b1\u51fd\u6570\u62e5\u6709\u5145\u5206\u7684\u4e86\u89e3\uff0c\u5b83\u4eec\u9884\u5148\u8ba1\u7b97\u6700\u4f18\u52a8\u4f5c\u6240\u9700\u7684\u6240\u6709\u4fe1\u606f\uff0c\u800c\u65e0\u9700\u5b9e\u9645\u91c7\u53d6\u4efb\u4f55\u884c\u52a8\u3002</p> <p>Next: \u5728\u7ebf\u89c4\u5212 \u5728\u5728\u7ebf\u89c4\u5212\u8fc7\u7a0b\u4e2d\uff0c\u4ee3\u7406\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5956\u52b1\u6216\u8f6c\u6362\u6ca1\u6709\u4efb\u4f55\u5148\u9a8c\u77e5\u8bc6\uff08\u4ecd\u7136\u4ee5\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b (MDP) \u7684\u5f62\u5f0f\u8868\u793a\uff09\u3002\u5728\u5728\u7ebf\u89c4\u5212\u4e2d\uff0c\u4ee3\u7406\u5fc5\u987b\u5c1d\u8bd5\u63a2\u7d22\uff0c\u5728\u6b64\u671f\u95f4\u5b83\u4f1a\u6267\u884c\u64cd\u4f5c\u5e76\u63a5\u6536\u53cd\u9988\uff0c\u53cd\u9988\u7684\u5f62\u5f0f\u5305\u62ec\u5b83\u5230\u8fbe\u7684\u540e\u7ee7\u72b6\u6001\u53ca\u5176\u83b7\u5f97\u7684\u76f8\u5e94\u5956\u52b1\u3002\u4ee3\u7406\u4f1a\u5229\u7528\u8fd9\u4e9b\u53cd\u9988\uff0c\u901a\u8fc7\u4e00\u4e2a RL \u6765\u4f30\u8ba1\u6700\u4f18\u7b56\u7565\uff0c\u7136\u540e\u518d\u5229\u7528\u8fd9\u4e2a\u4f30\u8ba1\u7684\u7b56\u7565\u8fdb\u884c\u5f00\u53d1\u6216\u5956\u52b1\u6700\u5927\u5316\u3002</p> <p>Setup - \u4ecd\u7136\u5047\u8bbe\u662f\u4e00\u4e2a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b (MDP)\uff1a     - \u4e00\u4e2a\u72b6\u6001\u96c6\u5408 \\(s \\in \\mathcal{S}\\)     - \u4e00\u4e2a\u884c\u52a8\u96c6\u5408\uff08\u6bcf\u4e2a\u72b6\u6001\uff09\\(\\mathcal{A}\\)     - \u4e00\u4e2a\u6a21\u578b \\(T(s, a, s')\\)     - \u4e00\u4e2a\u5956\u52b1\u51fd\u6570 \\(R(s, a, s')\\) - \u4ecd\u7136\u5728\u5bfb\u627e\u4e00\u4e2a\u7b56\u7565 \\(\\pi(s)\\)</p> <ul> <li>\u4e0d\u77e5\u9053 \\(T\\) \u6216 \\(R\\)</li> </ul>"},{"location":"Lecture%2009%2610%20RL/#1-model-based-learning","title":"1. Model-Based Learning","text":"<p> \u6839\u636e\u5927\u6570\u5b9a\u5f8b\uff0c\u968f\u7740\u6211\u4eec\u901a\u8fc7\u8ba9\u4ee3\u7406\u7ecf\u5386\u66f4\u591a\u573a\u666f\u6536\u96c6\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u6837\u672c\uff0c\u6211\u4eec\u7684 \\(\\hat{T}\\) \u548c \\(\\hat{R}\\) \u6a21\u578b\u5c06\u4f1a\u4e0d\u65ad\u6539\u8fdb\uff0c\u5176\u4e2d \\(\\hat{T}\\) \u4f1a\u6536\u655b\u5230 \\(T\\)\uff0c\u800c \\(\\hat{R}\\) \u5219\u4f1a\u968f\u7740\u6211\u4eec\u53d1\u73b0\u65b0\u7684 \\((s, a, s')\\) \u7ec4\u800c\u83b7\u53d6\u4e4b\u524d\u672a\u53d1\u73b0\u7684\u5956\u52b1\u77e5\u8bc6\u3002 \u5728\u5408\u9002\u7684\u65f6\u5019\u53ef\u4ee5\u7ed3\u675f\u4ee3\u7406\u7684\u8bad\u7ec3\uff0c\u901a\u8fc7\u4f7f\u7528\u5f53\u524d\u7684 \\(\\hat{T}\\) \u548c \\(\\hat{R}\\) \u6a21\u578b\u8fd0\u884c\u4ef7\u503c\u6216\u7b56\u7565\u8fed\u4ee3\u6765\u751f\u6210\u7b56\u7565 \\(\\pi_{exploit}\\)\uff0c\u5e76\u4f7f\u7528 \\(\\pi_{exploit}\\) \u8fdb\u884c\u5229\u7528\uff0c\u8ba9\u4ee3\u7406\u904d\u5386 MDP\uff0c\u91c7\u53d6\u884c\u52a8\u5bfb\u6c42\u5956\u52b1\u6700\u5927\u5316\u3002</p>"},{"location":"Lecture%2009%2610%20RL/#2-model-free-learning","title":"2. Model-Free Learning","text":"<p>Big Picture * Passive reinforcement learning: Direct evaluation and temporal difference learning   * \u5728\u88ab\u52a8\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u667a\u80fd\u4f53\u88ab\u8d4b\u4e88\u4e00\u4e2a\u7b56\u7565\uff0c\u5e76\u5728\u7ecf\u5386\u4e8b\u4ef6\u7684\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u8be5\u7b56\u7565\u4e0b\u72b6\u6001\u7684\u503c\uff0c\u8fd9\u6b63\u662f\u5f53 \\(T\\) \u548c \\(R\\) \u5df2\u77e5\u65f6\uff0cMDP \u7684\u7b56\u7565\u8bc4\u4f30\u6240\u505a\u7684\u4e8b\u60c5\u3002 * Active reinforcement learning: Q-learning   * \u5728\u6b64\u8fc7\u7a0b\u4e2d\uff0c\u5b66\u4e60\u667a\u80fd\u4f53\u53ef\u4ee5\u4f7f\u7528\u6536\u5230\u7684\u53cd\u9988\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u8fed\u4ee3\u66f4\u65b0\u5176\u7b56\u7565\uff0c\u76f4\u5230\u7ecf\u8fc7\u5145\u5206\u63a2\u7d22\u540e\u6700\u7ec8\u786e\u5b9a\u6700\u4f18\u7b56\u7565\u3002</p> <p>Regret: <code>\u4e00\u5f00\u59cb\u5c31\u5728\u73af\u5883\u4e2d\u91c7\u53d6\u6700\u4f18\u884c\u52a8\u6240\u79ef\u7d2f\u7684\u603b\u5956\u52b1</code>\u4e0e<code>\u901a\u8fc7\u8fd0\u884c\u5b66\u4e60\u7b97\u6cd5\u6240\u79ef\u7d2f\u7684\u603b\u5956\u52b1</code>\u4e4b\u95f4\u7684\u5dee\u5f02</p>"},{"location":"Lecture%2009%2610%20RL/#21-direct-evaluation-passive-rl","title":"2.1. Direct Evaluation (Passive RL)","text":"<p> \u8ba1\u7b97\u5f97\u5230 \\(V^\\pi(E) = -2\\) \u548c \\(V^\\pi(B) = 8\\)\uff0c\u4f46\u7406\u8bba\u4e0a \\(B\\) \u548c \\(E\\) \u5728 \\(\\pi\\) \u4e0b\u5e94\u8be5\u5177\u6709\u76f8\u540c\u7684\u503c\u3002</p> <p>\u8fd9\u662f\u56e0\u4e3a\u6211\u4eec\u7684\u4ee3\u7406\u5904\u4e8e\u72b6\u6001 \\(C\\) \u7684 4 \u6b21\u4e2d\uff0c\u5b83\u8f6c\u6362\u5230 \\(D\\) \u5e76\u83b7\u5f97\u4e86 3 \u6b21 10 \u7684\u5956\u52b1\uff0c\u8f6c\u6362\u5230 \\(A\\) \u5e76\u83b7\u5f97\u4e86 1 \u6b21 \\(-10\\) \u7684\u5956\u52b1\u3002\u7eaf\u5c5e\u5076\u7136\u7684\u662f\uff0c\u5f53\u5b83\u552f\u4e00\u4e00\u6b21\u83b7\u5f97 \\(-10\\) \u5956\u52b1\u65f6\uff0c\u5b83\u7684\u521d\u59cb\u72b6\u6001\u662f \\(E\\) \uff0c\u8fd9\u4e25\u91cd\u626d\u66f2\u4e86 \\(E\\) \u7684\u4f30\u503c\u3002</p> <p>\u7ecf\u8fc7\u8db3\u591f\u591a\u7684\u56de\u5408\u540e\uff0c\\(B\\) \u548c \\(E\\) \u7684\u503c\u5c06\u6536\u655b\u5230\u771f\u5b9e\u503c\uff0c\u4f46\u8fd9\u79cd\u60c5\u51b5\u4f1a\u5bfc\u81f4\u8be5\u8fc7\u7a0b\u8017\u65f6\u8d85\u8fc7\u6211\u4eec\u7684\u9884\u671f\u3002\u53ef\u4ee5\u901a\u8fc7\u9009\u62e9\u4f7f\u7528\u6211\u4eec\u7684\u7b2c\u4e8c\u79cd\u88ab\u52a8\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u2014\u2014\u65f6\u95f4\u5dee\u5206\u5b66\u4e60\u6765\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\u3002</p>"},{"location":"Lecture%2009%2610%20RL/#22-temporal-difference-learning-passive-rl","title":"2.2. Temporal Difference Learning (Passive RL)","text":"<p>TD Learning \u91c7\u7528\u4ece\u6bcf\u4e00\u6b21\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u7684\u7406\u5ff5\uff0c\u800c\u4e0d\u662f\u50cf Direct Evaluation \u90a3\u6837\u7b80\u5355\u5730\u8ddf\u8e2a\u603b\u5956\u52b1\u548c\u72b6\u6001\u8bbf\u95ee\u6b21\u6570\uff0c\u5e76\u5728\u6700\u540e\u8fdb\u884c\u5b66\u4e60\u3002</p> <p>Sample of V(s): $$ \\text{sample} = R(s, \\pi(s), s') + \\gamma V^\\pi(s') $$</p> <p>Update to V(s): $$ V^\\pi(s) \\leftarrow (1 - \\alpha)V^\\pi(s) + (\\alpha)\\text{sample} $$</p> <p>\u5176\u4e2d learning rate \\(\\alpha \\in [0,1]\\)\uff0c\u4e00\u822c\u4ece 1 \u6162\u6162\u7f29\u5c0f\u5230 0.</p>"},{"location":"Lecture%2009%2610%20RL/#23-q-learning-active-rl","title":"2.3. Q-Learning (Active RL)","text":"<p>TD Learning \u6216 Direct Learning \u901a\u5e38\u4e0e\u4e00\u4e9b\u57fa\u4e8e\u6a21\u578b\u7684\u5b66\u4e60\u7ed3\u5408\u4f7f\u7528\uff0c\u4ee5\u83b7\u53d6 \\(T\\) \u548c \\(R\\) \u7684\u4f30\u8ba1\u503c\uff0c\u4ece\u800c\u6709\u6548\u5730\u66f4\u65b0\u5b66\u4e60\u4ee3\u7406\u6240\u9075\u5faa\u7684\u7b56\u7565\u3002Q Learning \u63d0\u51fa\u76f4\u63a5\u5b66\u4e60\u72b6\u6001\u7684 \\(Q\\) \u503c\uff0c\u65e0\u9700\u4e86\u89e3\u4efb\u4f55\u503c\u3001\u8f6c\u6362\u51fd\u6570\u6216\u5956\u52b1\u51fd\u6570\u3002\u56e0\u6b64\uff0c\\(Q\\) \u5b66\u4e60\u5b8c\u5168\u65e0\u9700\u6a21\u578b\uff08off-policy learning\uff09\u3002</p> <p>\\(Q\\) \u5b66\u4e60\u4f7f\u7528\u4ee5\u4e0b\u66f4\u65b0\u89c4\u5219\u6765\u6267\u884c\u6240\u8c13\u7684 \\(Q\\) \u503c\u8fed\u4ee3\uff1a</p> \\[ Q_{k+1}(s, a) \\leftarrow \\sum_{s'} T(s, a, s')[R(s, a, s') + \\gamma \\max_{a'} Q_k(s', a')] $$ - Learn $Q(s, a)$ values as you go     - Receive a sample $(s, a, s', r)$     - Consider your old estimate: $Q(s, a)$     - Consider your new sample estimate:     $$     \\text{sample} = R(s, a, s') + \\gamma \\max_{a'} Q(s', a')     $$     - Incorporate the new estimate into a running average:     $$     Q(s, a) \\leftarrow (1 - \\alpha)Q(s, a) + (\\alpha) [\\text{sample}]     \\]"},{"location":"Lecture%2009%2610%20RL/#24-approximate-q-learning","title":"2.4. Approximate Q-Learning","text":"<p>Q-Learning \u5360\u7528\u5185\u5b58\u592a\u5927\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u7279\u5f81\u5411\u91cf\u7b80\u5316\uff1a $$ V(s) = w_1f_1(s) + w_2f_2(s) + \\dots + w_nf_n(s) $$</p> \\[ Q(s, a) = w_1f_1(s, a) + w_2f_2(s, a) + \\dots + w_nf_n(s, a) $$ $$ \\text{difference} = \\left[r + \\gamma \\max_{a'} Q(s', a')\\right] - Q(s, a) \\] \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha [\\text{difference}] \\] \\[ w_i \\leftarrow w_i + \\alpha [\\text{difference}] f_i(s, a) \\]"},{"location":"Lecture%2009%2610%20RL/#3-exploration-vs-exploitation-vs","title":"3. Exploration vs. Exploitation / \u63a2\u7d22 vs. \u5229\u7528","text":"<ul> <li>Exploration - \u901a\u8fc7\u5c1d\u8bd5\u4e0d\u540c\u7684\u884c\u4e3a\u6765\u5f97\u5230\u4e00\u4e2a\u6700\u4f73\u7684\u7b56\u7565\uff0c\u5f97\u5230\u6700\u5927\u5956\u52b1\u7684\u7b56\u7565\u3002</li> <li>Exploitation - \u4e0d\u53bb\u5c1d\u8bd5\u65b0\u7684\u4e1c\u897f\uff0c\u5c31\u91c7\u53d6\u5df2\u77e5\u7684\u53ef\u4ee5\u5f97\u5230\u5f88\u5927\u5956\u52b1\u7684\u884c\u4e3a\u3002</li> </ul>"},{"location":"Lecture%2009%2610%20RL/#31-epsilon-greedy","title":"3.1. \\(\\epsilon\\) Greedy","text":"<p>\u9075\u5faa \\(\\epsilon\\) Greedy \u7684\u4ee3\u7406\u5b9a\u4e49\u4e86\u67d0\u4e2a\u6982\u7387 \\(0 \\le \\epsilon \\le 1\\)\uff0c\u5e76\u4e14\u4f1a\u4ee5\u6982\u7387 \\(\\epsilon\\) \u968f\u673a\u884c\u52a8\u548c\u63a2\u7d22\u3002</p> <p>\u5982\u679c\u4e3a \\(\\epsilon\\) \u9009\u62e9\u4e86\u4e00\u4e2a\u8f83\u5927\u7684\u503c\uff0c\u90a3\u4e48\u5373\u4f7f\u5728\u5b66\u4e60\u4e86\u6700\u4f18\u7b56\u7565\u4e4b\u540e\uff0c\u4ee3\u7406\u4ecd\u7136\u4f1a\u4ee5\u968f\u673a\u7684\u65b9\u5f0f\u884c\u4e8b\u3002\u540c\u6837\uff0c\u4e3a \\(\\epsilon\\) \u9009\u62e9\u4e86\u4e00\u4e2a\u8f83\u5c0f\u7684\u503c\u610f\u5473\u7740\u4ee3\u7406\u5c06\u5f88\u5c11\u8fdb\u884c\u63a2\u7d22\uff0c\u4ece\u800c\u5bfc\u81f4 \\(Q\\) \u5b66\u4e60\uff08\u6216\u4efb\u4f55\u5176\u4ed6\u9009\u5b9a\u7684\u5b66\u4e60\u7b97\u6cd5\uff09\u975e\u5e38\u7f13\u6162\u5730\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u3002</p> <p>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5fc5\u987b\u624b\u52a8\u8c03\u6574 \\(\\epsilon\\) \u5e76\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u964d\u4f4e\u624d\u80fd\u770b\u5230\u7ed3\u679c\u3002</p>"},{"location":"Lecture%2009%2610%20RL/#32-exploration-function","title":"3.2. Exploration Function","text":"\\[ Q(s, a) \\leftarrow (1 - \\alpha)Q(s, a) + \\alpha \\cdot [R(s, a, s') + \\gamma \\max_{a'} f(s', a')] \\] <p>\u5176\u4e2d \\(f\\) \u8868\u793a\u63a2\u7d22\u51fd\u6570\u3002\u8bbe\u8ba1\u63a2\u7d22\u51fd\u6570\u6709\u4e00\u5b9a\u7684\u7075\u6d3b\u6027\uff0c\u4f46\u901a\u5e38\u7684\u9009\u62e9\u662f\u4f7f\u7528\uff1a</p> \\[ f(s, a) = Q(s, a) + \\frac{k}{N(s, a)} \\]"},{"location":"Lecture%2009%2610%20RL/#_1","title":"Lecture 09&10 RL","text":""},{"location":"Lecture%2011-14%20Probability%20and%20Bayes%20Nets/","title":"Lecture 11-14 Probability and Bayes Nets","text":""},{"location":"Lecture%2011-14%20Probability%20and%20Bayes%20Nets/#cs-180-introduction-to-ai","title":"CS 180 - Introduction to AI","text":""},{"location":"Lecture%2011-14%20Probability%20and%20Bayes%20Nets/#lecture-11-14-probability-and-bayes-nets","title":"Lecture 11-14: Probability and Bayes Nets","text":""},{"location":"Lecture%2011-14%20Probability%20and%20Bayes%20Nets/#1-probability-recap","title":"1. Probability Recap","text":"<ul> <li> <p>Conditional probability \\(\\(P(x|y) = \\frac{P(x, y)}{P(y)}\\)\\)</p> </li> <li> <p>Product rule \\(\\(P(x, y) = P(x|y)P(y)\\)\\)</p> </li> <li> <p>Chain rule \\(\\(P(X_1, X_2, \\dots, X_n) = P(X_1)P(X_2|X_1)P(X_3|X_1, X_2)\\dots\\)\\) \\(\\(= \\prod_{i=1}^{n} P(X_i|X_1, \\dots, X_{i-1})\\)\\)</p> </li> <li> <p>X, Y independent if and only if: \\(\\(\\forall x, y : P(x, y) = P(x)P(y)\\)\\)</p> </li> <li> <p>X and Y are conditionally independent given Z if and only if: \\(\\(\\forall x, y, z : P(x, y|z) = P(x|z)P(y|z) \\qquad X \\perp Y|Z\\)\\)</p> </li> </ul>"},{"location":"Lecture%2011-14%20Probability%20and%20Bayes%20Nets/#2-bayesian-network-representation","title":"2. Bayesian Network Representation","text":"<p>E.g.\u8003\u8651\u4e00\u4e2a\u6a21\u578b\uff0c\u5176\u4e2d\u6211\u4eec\u6709\u5982\u4e0b\u4e94\u4e2a\u4e8c\u8fdb\u5236\u968f\u673a\u53d8\u91cf\uff1a</p> <ul> <li>B: Burglary occurs.</li> <li>A: Alarm goes off.</li> <li>E: Earthquake occurs.</li> <li>J: John calls.</li> <li>M: Mary calls.</li> </ul> <p></p> <p>\u8d1d\u53f6\u65af\u7f51\u7edc\u56fe\u7684\u7ed3\u6784\u7f16\u7801\u4e86\u4e0d\u540c\u8282\u70b9\u4e4b\u95f4\u7684\u6761\u4ef6\u72ec\u7acb\u5173\u7cfb\u3002</p> <p>\u8d1d\u53f6\u65af\u7f51\u7edc\u8282\u70b9\u4e4b\u95f4\u7684\u8fb9\u5e76\u4e0d\u610f\u5473\u7740\u8fd9\u4e9b\u8282\u70b9\u4e4b\u95f4\u5b58\u5728\u7279\u5b9a\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u4e5f\u4e0d\u610f\u5473\u7740\u53d8\u91cf\u4e4b\u95f4\u5fc5\u7136\u76f8\u4e92\u4f9d\u8d56\u3002\u5b83\u53ea\u662f\u610f\u5473\u7740\u8282\u70b9\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u67d0\u79cd\u5173\u7cfb\u3002</p>"}]}